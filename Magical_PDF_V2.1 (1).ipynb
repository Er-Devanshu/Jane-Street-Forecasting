{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Magical PDF V2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -- IMPORTS --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pdfminer.layout import LAParams, LTTextBoxHorizontal, LTTextBoxVertical, LTTextLineHorizontal, LTTextLineVertical, LTChar, LTTextContainer,LTAnno, LTFigure, LTImage, LTRect, LTLine, LTPage,LTTextLine\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import PDFPageAggregator\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.high_level import extract_pages\n",
    "import re\n",
    "import fitz  # PyMuPDF\n",
    "import os\n",
    "from rapidfuzz import fuzz\n",
    "import io\n",
    "import cv2 \n",
    "import numpy as np\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from spellchecker import SpellChecker\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -- PATTERNS --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [\n",
    "    # Patterns for \"Section\"\n",
    "    r\"^section\\s\\d{2}\\s\\d{2}\\s\\d{2}\",\n",
    "    r\"^section—\\d+\",\n",
    "    r\"^section\\s\\d+\",\n",
    "    r\"^Section\\s\\d{2}\\s\\d{2}\\s\\d{2}\",\n",
    "    r\"^Section—\\d+\",\n",
    "    r\"^Section\\s\\d+\",\n",
    "    r\"^SECTION\\s\\d{2}\\s\\d{2}\\s\\d{2}\",\n",
    "    r\"^SECTION—\\d+\",\n",
    "    r\"^SECTION\\s\\d+\",\n",
    "\n",
    "    # Patterns for \"Chapter\"\n",
    "    r\"^chapter—\\d+\",\n",
    "    r\"^chapter\\s\\d+\",\n",
    "    r\"^Chapter—\\d+\",\n",
    "    r\"^Chapter\\s\\d+\",\n",
    "    r\"^CHAPTER—\\d+\",\n",
    "    r\"^CHAPTER\\s\\d+\",\n",
    "    # Patters for Exhibit\n",
    "    r\"^Exhibit\\s[A-Z]\\s\",\n",
    "    r\"^Exhibit\\s\\d+\\s\",\n",
    "    # Patterns for \"Part\"\n",
    "    r\"^part\\s\\d\",\n",
    "    r\"^Part\\s\\d\",\n",
    "    r\"^PART\\s\\d\",\n",
    "\n",
    "    # Patterns for \"Step\"\n",
    "    r\"^Step\\s\\d\\:\",\n",
    "    r\"^STEP\\s\\d\\:\",\n",
    "    r\"^step\\s\\d\\:\",\n",
    "\n",
    "    # Patterns for \"Appendix\"\n",
    "    r\"^Appendix\\s[a-z]\\s\",\n",
    "    r\"^APPENDIX\\s[a-z]\\s\",\n",
    "    r\"^appendix\\s[a-z]\\s\",\n",
    "\n",
    "    # Patterns for \"Article\"\n",
    "    r\"^Article\\s\\d\\.\\d\\d\\s\",\n",
    "    r\"^Article\\s\\d\",\n",
    "    r\"^ARTICLE\\s\\d\",\n",
    "    r\"^article\\s\\d\",\n",
    "\n",
    "    # Patterns for \"Schedule\"\n",
    "    r\"^SCHEDULE\\s[A-Z]\\s\",\n",
    "\n",
    "    # Patterns for Roman Enumeral\n",
    "    r\"^\\s*(I|II|III|IV|V|VI|VII|VIII|IX|X|XI|XII|XIII|XIV|XV|XVI|XVII|XVIII|XIX|XX)\\.\\s*\",  # \"I., II.\"\n",
    "    r\"^\\s*(I|II|III|IV|V|VI|VII|VIII|IX|X|XI|XII|XIII|XIV|XV|XVI|XVII|XVIII|XIX|XX)\\)\\s*\",  # \"I), II)\"\n",
    "    r\"^\\s*\\((I|II|III|IV|V|VI|VII|VIII|IX|X|XI|XII|XIII|XIV|XV|XVI|XVII|XVIII|XIX|XX)\\)\\s*\",# \"(I), (II)\"\n",
    "    r\"^\\s*(i|ii|iii|iv|v|vi|vii|viii|ix|x|xi|xii|xiii|xiv|xv|xvi|xvii|xviii|xix|xx)\\.\\s*\",  # \"i., ii.\"\n",
    "    r\"^\\s*(i|ii|iii|iv|v|vi|vii|viii|ix|x|xi|xii|xiii|xiv|xv|xvi|xvii|xviii|xix|xx)\\)\\s*\",  # \"i), ii)\"\n",
    "    r\"^\\s*\\((i|ii|iii|iv|v|vi|vii|viii|ix|x|xi|xii|xiii|xiv|xv|xvi|xvii|xviii|xix|xx)\\)\\s*\",# \"(i), (ii)\"\n",
    "\n",
    "    # Others: i, v, and x are excluded on purpose\n",
    "    r\"^\\(\\s*([A-RTUWYZ])\\s*\\)\",   # \"(H)\" & \" (H) \"\n",
    "    r\"^(?!I\\.)[A-RTUWYZ]\\.\\s\",    # \"H. \"\n",
    "    r\"^([A-RTUWYZ])\\)\\s\",         # \"H) \"\n",
    "    r\"^[A-RTUWYZ]\\.\\s\",           # \"H. \"\n",
    "    r\"^[A-RTUWYZ][A-RTUWYZ]\\.\\s\", # \"HH. \"\n",
    "    r\"^\\(\\s*([a-rtuwyz])\\s*\\)\",   # \"(h)\" & \" (h) \"\n",
    "    r\"^(?!i\\.)[a-rtuwyz]\\.\\s\",    # \"h. \"\n",
    "    r\"^([a-rtuwyz])\\)\\s\",         # \"h) \"\n",
    "    r\"^[a-rtuwyz]\\.\\s\",           # \"h. \"\n",
    "    r\"^[a-rtuwyz][a-rtuwyz]\\.\\s\", # \"hh. \"\n",
    "    # Patterns with Letters\n",
    "    r\"^[A-Z]\\.\\d+\\s\",\n",
    "    r\"^[A-Z]\\.\\d+\\.\\d+\\s\",\n",
    "    r\"^[A-Z]\\.\\d+\\.\\d+\\.\\d+\\s\",\n",
    "    r\"^[A-Z]\\.\\d+\\.\\d+\\.\\d+\\.\\d+\\s\",\n",
    "    \n",
    "    # Patterns with Numbers\n",
    "    r\"^\\d{2}\\s\\d{2}\\s\\d{2}\",\n",
    "    r\"^\\d{1,5}\\.\\s\",\n",
    "    r\"^\\d{1,5}\\.\\d{1,5}\\s\",\n",
    "    r\"^\\d{1,5}\\.\\d{1,5}.\\s\",\n",
    "    r\"^[a-z]\\s\\d{1,5}\\.\\d{1,5}\\s\",\n",
    "    r\"^[A-Z]\\s\\d{1,5}\\.\\d{1,5}\\s\",\n",
    "    r\"^\\d{1,5}\\.\\d{1,5}\\*\\s\",\n",
    "    r\"^\\d{1,5}\\.\\d{1,5}\\.\\d{1,5}\\s\",\n",
    "    r\"^\\d{1,5}\\.\\d{1,5}\\.\\d{1,5}.\\s\",\n",
    "    r\"^\\Δ\\s\\d{1,5}\\.\\d{1,5}\\.\\d{1,5}\\s\",\n",
    "    r\"^[a-z]\\s\\d{1,5}\\.\\d{1,5}\\.\\d{1,5}\\s\",\n",
    "    r\"^[A-Z]\\s\\d{1,5}\\.\\d{1,5}\\.\\d{1,5}\\s\",\n",
    "    r\"^\\Δ\\s\\d{1,5}\\.\\d{1,5}\\.\\d{1,5}\\*\\s\",\n",
    "    r\"^\\d{1,5}\\.\\d{1,5}\\.\\d{1,5}\\*\\s\",\n",
    "    r\"^\\d{1,5}\\.\\d{1,5}\\.\\d{1,5}\\.\\d{1,5}\\s\",\n",
    "    r\"^\\d{1,5}\\.\\d{1,5}\\.\\d{1,5}\\.\\d{1,5}.\\s\",\n",
    "    r\"^[a-z]\\s\\d{1,5}\\.\\d{1,5}\\.\\d{1,5}\\.\\d{1,5}\\s\",\n",
    "    r\"^[A-Z]\\s\\d{1,5}\\.\\d{1,5}\\.\\d{1,5}\\.\\d{1,5}\\s\",\n",
    "    r\"^\\Δ\\s\\d{1,5}\\.\\d{1,5}\\.\\d{1,5}\\.\\d{1,5}\\s\",\n",
    "    r\"^\\Δ\\s\\d{1,5}\\.\\d{1,5}\\.\\d{1,5}\\.\\d{1,5}\\*\\s\",\n",
    "    r\"^\\d{1,5}\\.\\d{1,5}\\.\\d{1,5}\\.\\d{1,5}\\*\\s\",\n",
    "    r\"^\\d{1,5}\\.\\d{1,5}\\.\\d{1,5}\\.\\d{1,5}\\.\\d{1,5}\\s\",\n",
    "    r\"^\\d{1,5}\\.\\d{1,5}\\.\\d{1,5}\\.\\d{1,5}\\.\\d{1,5}.\\s\",\n",
    "    r\"^\\d{1,5}\\.\\d{1,5}\\.\\d{1,5}\\.\\d{1,5}\\.\\d{1,5}\\*\\s\",\n",
    "    r\"^\\d{1,5}\\)\\s\",\n",
    "    r\"^\\d{1,5}\\s\",\n",
    "    r\"^\\d{1,5}\\*\\s\",\n",
    "    r\"^\\[\\d{1,5}\\]\",\n",
    "    r\"^\\d{1,5}\\—\",\n",
    "    r\"^\\d{1,5}\\.\\—\",\n",
    "    r\"^\\d{1,5}\\.\\d{1,5}\\—\",\n",
    "    r\"^\\d{1,5}\\.\\d{1,5}\\.\\d{1,5}\\—\",\n",
    "    r\"^\\d{1,5}\\.\\d{1,5}\\.\\d{1,5}\\.\\d{1,5}\\—\",\n",
    "    r\"^\\d{1,5}\\.\\d{1,5}\\.\\d{1,5}\\.\\d{1,5}\\.\\d{1,5}\\—\",\n",
    "    r\"^\\(\\d+\\)\",\n",
    "\n",
    "    #### next patterns used to split Section and PT ###\n",
    "    r\"^b\\[\\d+\\]\\s\",\n",
    "    r\"^B\\[\\d+\\]\\s\",\n",
    "    r\"^\\[Table\\/Figure\\]\\s\",\n",
    "    r\"^\\[table\\/figure\\]\\s\",\n",
    "    r\"^P\\[\\d+\\]\\s\",\n",
    "    r\"^p\\[\\d+\\]\\s\",\n",
    "]\n",
    "\n",
    "bullet_patterns = [\n",
    "    r\"^\\W\\s\",  # Any non-alphanumeric character followed by a space\n",
    "    r\"^\\•\\s\",\n",
    "    r\"^\\-\\s\",\n",
    "    r\"^\\*\\s\",\n",
    "    r\"^\\\"\\s\",\n",
    "    r\"^\\'\\s\",\n",
    "    r\"^\\u2022\\s\",  # Bullet point (•)\n",
    "    r\"^\\u25AA\\s\",  # Black small square (▪)\n",
    "    r\"^\\u25AB\\s\",  # White small square (▫)\n",
    "    r\"^\\u25A0\\s\",  # Black square (■)\n",
    "    r\"^\\u25A1\\s\",  # White square (□)\n",
    "    r\"^\\u25B2\\s\",  # Black up-pointing triangle (▲)\n",
    "    r\"^\\u25B3\\s\",  # White up-pointing triangle (△)\n",
    "    r\"^\\u25BC\\s\",  # Black down-pointing triangle (▼)\n",
    "    r\"^\\u25BD\\s\",  # White down-pointing triangle (▽)\n",
    "    r\"^\\u25C6\\s\",  # Black diamond (◆)\n",
    "    r\"^\\u25C7\\s\",  # White diamond (◇)\n",
    "    r\"^\\u25CF\\s\",  # Black circle (●)\n",
    "    r\"^\\u25CB\\s\",  # White circle (○)\n",
    "    r\"^\\u25D8\\s\",  # Inverse bullet (◘)\n",
    "    r\"^\\u25D9\\s\",  # Inverse white circle (◙)\n",
    "    r\"^\\u25E6\\s\",  # White bullet (◦)\n",
    "    r\"^\\u2756\\s\",  # Diamond with a question mark inside (❖)\n",
    "    r\"^\\uF0E0\\s\",  # Special bullet character ()\n",
    "    r\"^\\uf07d\\s\",\n",
    "]\n",
    "\n",
    "table_figure_patterns = [\n",
    "    r\"^table\\s\\d+\",             \n",
    "    r\"^table\\s\\d+:\\s\",          \n",
    "    r\"^table\\s\\d+\\.\\d+\",        \n",
    "    r\"^figure\\s\\d+\",            \n",
    "    r\"^figure\\s\\d+\\.\\d+\",\n",
    "    r\"^Table\\s\\d+\",             \n",
    "    r\"^Table\\s\\d+:\\s\",          \n",
    "    r\"^Table\\s\\d+\\.\\d+\",        \n",
    "    r\"^Figure\\s\\d+\",            \n",
    "    r\"^Figure\\s\\d+\\.\\d+\",\n",
    "    r\"^TABLE\\s\\d+\",             \n",
    "    r\"^TABLE\\s\\d+:\\s\",          \n",
    "    r\"^TABLE\\s\\d+\\.\\d+\",        \n",
    "    r\"^FIGURE\\s\\d+\",            \n",
    "    r\"^FIGURE\\s\\d+\\.\\d+\",      \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -- OCR ENGINE --> PDF to Brut Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Run PDF Miner on PDF to extract text from PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recalculate_bounding_box(chars):\n",
    "    x0, y0, x1, y1 = None, None, None, None\n",
    "    for char in chars:\n",
    "        if isinstance(char, LTChar):\n",
    "            char_x0, char_y0, char_x1, char_y1 = char.bbox\n",
    "            if x0 is None or char_x0 < x0:\n",
    "                x0 = char_x0\n",
    "            if y0 is None or char_y0 < y0:\n",
    "                y0 = char_y0\n",
    "            if x1 is None or char_x1 > x1:\n",
    "                x1 = char_x1\n",
    "            if y1 is None or char_y1 > y1:\n",
    "                y1 = char_y1\n",
    "    return x0, y0, x1, y1\n",
    "\n",
    "def rebuild_lineboxes(document_path, laparams): # Rebuild PdfMiner Line Boxes Based on Characters Coordinates\n",
    "    lineboxes_data = []\n",
    "    previous_x0, previous_y0, previous_x1, previous_y1 = None, None, None, None\n",
    "\n",
    "    # Extract pages with custom laparams\n",
    "    pages = list(extract_pages(document_path, laparams=laparams))\n",
    "    for page_number, page in enumerate(tqdm(pages, desc=\"Processing Pages for Lineboxes\"), start=1):\n",
    "        # Check and rotate page if necessary\n",
    "        if page.width > page.height:\n",
    "            # Page is in landscape mode; rotate it\n",
    "            page.rotate = 90\n",
    "        for element in page:\n",
    "            if isinstance(element, LTTextBoxHorizontal):\n",
    "                for text_line in element:\n",
    "                    chars = []\n",
    "                    for char in text_line:\n",
    "                        if isinstance(char, LTChar):\n",
    "                            chars.append(char)\n",
    "                    if chars:\n",
    "                        x0, y0, x1, y1 = recalculate_bounding_box(chars)\n",
    "                        line_text = ''.join([char.get_text() for char in chars])\n",
    "                        \n",
    "                        if previous_x0 is not None and previous_y0 is not None:\n",
    "                            x_gap = abs(round(x0 - previous_x0, 2))\n",
    "                            y_gap = abs(round(y0 - previous_y0, 2))\n",
    "                        else:\n",
    "                            x_gap, y_gap = None, None\n",
    "\n",
    "                        lineboxes_data.append({\n",
    "                            'Page': page_number,\n",
    "                            'Text': line_text.strip(),\n",
    "                            'x0': round(x0, 2),\n",
    "                            'y0': round(y0, 2),\n",
    "                            'x1': round(x1, 2),\n",
    "                            'y1': round(y1, 2),\n",
    "                            'x_gap': x_gap,\n",
    "                            'y_gap': y_gap\n",
    "                        })\n",
    "\n",
    "                        previous_x0, previous_y0, previous_x1, previous_y1 = x0, y0, x1, y1\n",
    "    \n",
    "    df = pd.DataFrame(lineboxes_data)\n",
    "    return df\n",
    "\n",
    "def CreateDataFrameFromPDF(document_path): # Main PDF Miner Funcion & Initiate PDF Miner LAParams \n",
    "    laparams = LAParams(boxes_flow=None)\n",
    "    # Rebuild lineboxes based on characters\n",
    "    df = rebuild_lineboxes(document_path, laparams)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Run Tesseract OCR on text not detected by PDF Miner, Higlight findinds in PDF and consolidate Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def redact_combined_text(pdf_path, combined_df, output_pdf_path, problematic_pages=None, dpi_value=72): # Redact PDF with white boxes over text already detected by PDF Miner\n",
    "    doc = fitz.open(pdf_path)\n",
    "    dpi_scale = dpi_value / 72  # Adjust scaling if needed\n",
    "    padding = 0\n",
    "    # Default to empty list if no problematic pages specified\n",
    "    if problematic_pages is None:\n",
    "        problematic_pages = [page_num + 1 for page_num in range(len(doc)) if doc[page_num].rotation != 0]\n",
    "\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc[page_num]\n",
    "        # Get page dimensions\n",
    "        page_width = page.mediabox_size[0]\n",
    "        page_height = page.mediabox_size[1]\n",
    "\n",
    "        # Filter DataFrame for the current page\n",
    "        page_df = combined_df[combined_df['Page'] == page_num + 1]\n",
    "\n",
    "        for _, row in page_df.iterrows():\n",
    "            x0 = row['x0'] - padding\n",
    "            y0 = row['y0'] - padding\n",
    "            x1 = row['x1'] + padding\n",
    "            y1 = row['y1'] + padding\n",
    "\n",
    "            # Adjust y-coordinates for PyMuPDF coordinate system\n",
    "            y0_pdf = page_height - y1\n",
    "            y1_pdf = page_height - y0\n",
    "\n",
    "            if (page_num + 1) in problematic_pages:\n",
    "                # Use swapped coordinates for problematic pages\n",
    "                rect = fitz.Rect(y0, x0, y1, x1)\n",
    "            else:\n",
    "                # Use standard coordinates for normal pages\n",
    "                rect = fitz.Rect(x0, y0_pdf, x1, y1_pdf)\n",
    "            \n",
    "            # Add the redaction annotation with a white fill\n",
    "            page.add_redact_annot(rect, fill=(1, 1, 1))\n",
    "\n",
    "        # Apply the redactions to the page\n",
    "        page.apply_redactions()\n",
    "\n",
    "    # Save the redacted PDF\n",
    "    doc.save(output_pdf_path)\n",
    "    print(f\"Redacted PDF saved as '{output_pdf_path}'.\")\n",
    "\n",
    "def preprocess_image_1(image_bytes, gamma = 2.3): # PDF Preprossessing\n",
    "    # Gamma Correction\n",
    "    image = Image.open(io.BytesIO(image_bytes))\n",
    "    open_cv_image = np.array(image)\n",
    "\n",
    "    # Convert to float and normalize\n",
    "    img_float = open_cv_image.astype(np.float32) / 255.0\n",
    "\n",
    "    # Apply gamma correction\n",
    "    gamma_corrected = np.power(img_float, gamma)\n",
    "\n",
    "    # Convert back to uint8\n",
    "    gamma_corrected = np.uint8(gamma_corrected * 255)\n",
    "\n",
    "    if len(gamma_corrected.shape) == 3:\n",
    "        gray = cv2.cvtColor(gamma_corrected, cv2.COLOR_RGB2GRAY)\n",
    "    else:\n",
    "        gray = gamma_corrected\n",
    "    filtered_image = cv2.bilateralFilter(gray, d=9, sigmaColor=75, sigmaSpace=75)\n",
    "\n",
    "    # Thresholding\n",
    "    _, binary = cv2.threshold(filtered_image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "    return binary\n",
    "def preprocess_image_2(image_bytes, gamma = 3.5): # PDF Preprossessing\n",
    "    # Gamma Correction\n",
    "    image = Image.open(io.BytesIO(image_bytes))\n",
    "    open_cv_image = np.array(image)\n",
    "\n",
    "    # Convert to float and normalize\n",
    "    img_float = open_cv_image.astype(np.float32) / 255.0\n",
    "\n",
    "    # Apply gamma correction\n",
    "    gamma_corrected = np.power(img_float, gamma)\n",
    "\n",
    "    # Convert back to uint8\n",
    "    gamma_corrected = np.uint8(gamma_corrected * 255)\n",
    "\n",
    "    if len(gamma_corrected.shape) == 3:\n",
    "        gray = cv2.cvtColor(gamma_corrected, cv2.COLOR_RGB2GRAY)\n",
    "    else:\n",
    "        gray = gamma_corrected\n",
    "    filtered_image = cv2.bilateralFilter(gray, d=9, sigmaColor=75, sigmaSpace=75)\n",
    "\n",
    "    # Thresholding\n",
    "    _, binary = cv2.threshold(filtered_image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "    return binary\n",
    "def preprocess_image_3(image_bytes, gamma = 3.5): # PDF Preprossessing\n",
    "    # Gamma Correction\n",
    "    image = Image.open(io.BytesIO(image_bytes))\n",
    "    open_cv_image = np.array(image)\n",
    "\n",
    "    # Convert to float and normalize\n",
    "    img_float = open_cv_image.astype(np.float32) / 255.0\n",
    "\n",
    "    # Apply gamma correction\n",
    "    gamma_corrected = np.power(img_float, gamma)\n",
    "\n",
    "    # Convert back to uint8\n",
    "    gamma_corrected = np.uint8(gamma_corrected * 255)\n",
    "\n",
    "    if len(gamma_corrected.shape) == 3:\n",
    "        gray = cv2.cvtColor(gamma_corrected, cv2.COLOR_RGB2GRAY)\n",
    "    else:\n",
    "        gray = gamma_corrected\n",
    "\n",
    "    # Thresholding\n",
    "    _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    return binary\n",
    "def preprocess_image_4(image_bytes, gamma = 2.0): # PDF Preprossessing\n",
    "    # Gamma Correction\n",
    "    image = Image.open(io.BytesIO(image_bytes))\n",
    "    open_cv_image = np.array(image)\n",
    "\n",
    "    # Convert to float and normalize\n",
    "    img_float = open_cv_image.astype(np.float32) / 255.0\n",
    "\n",
    "    # Apply gamma correction\n",
    "    gamma_corrected = np.power(img_float, gamma)\n",
    "\n",
    "    # Convert back to uint8\n",
    "    gamma_corrected = np.uint8(gamma_corrected * 255)\n",
    "\n",
    "    if len(gamma_corrected.shape) == 3:\n",
    "        gray = cv2.cvtColor(gamma_corrected, cv2.COLOR_RGB2GRAY)\n",
    "    else:\n",
    "        gray = gamma_corrected\n",
    "    filtered_image = cv2.bilateralFilter(gray, d=9, sigmaColor=75, sigmaSpace=75)\n",
    "\n",
    "    # Thresholding\n",
    "    _, binary = cv2.threshold(filtered_image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "    return binary\n",
    "\n",
    "def remove_watermark(image_bytes, padding=3): # Function to remove watermarks\n",
    "    image = Image.open(io.BytesIO(image_bytes))\n",
    "    open_cv_image = np.array(image)\n",
    "    # Convert to grayscale\n",
    "    if len(open_cv_image.shape) == 3:\n",
    "        gray_image = cv2.cvtColor(open_cv_image, cv2.COLOR_RGB2GRAY)\n",
    "    else:\n",
    "        gray_image = open_cv_image\n",
    "    # Invert the image\n",
    "    inverted_image = cv2.bitwise_not(gray_image)\n",
    "\n",
    "    # Apply thresholding to keep only solid black areas\n",
    "    _, thresholded_image = cv2.threshold(inverted_image, 125, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Apply morphological operations to remove noise and expand black regions\n",
    "    kernel = np.ones((padding, padding), np.uint8)\n",
    "    padded_image = cv2.dilate(thresholded_image, kernel, iterations=1)\n",
    "    \n",
    "    # Convert back to image bytes\n",
    "    final_image = cv2.bitwise_not(padded_image)\n",
    "    pil_image = Image.fromarray(final_image)\n",
    "    img_byte_arr = io.BytesIO()\n",
    "    pil_image.save(img_byte_arr, format='PNG')\n",
    "    processed_image_bytes = img_byte_arr.getvalue()\n",
    "\n",
    "    return processed_image_bytes\n",
    "def make_near_white_pixels_white(img_bytes, threshold=180):\n",
    "    \"\"\"\n",
    "    Convert near-white pixels to pure white.\n",
    "    threshold: The lower bound for pixel values considered 'nearly white'.\n",
    "               Any pixel with all channels > threshold will be set to pure white.\n",
    "    \"\"\"\n",
    "    # Open image from bytes\n",
    "    img = Image.open(io.BytesIO(img_bytes)).convert(\"RGB\")\n",
    "    arr = np.array(img)\n",
    "    \n",
    "    # Create a mask for near-white pixels\n",
    "    # For a pixel to be considered near-white, all its channels should be above `threshold`\n",
    "    near_white_mask = (arr[:,:,0] > threshold) & (arr[:,:,1] > threshold) & (arr[:,:,2] > threshold)\n",
    "    \n",
    "    # Set these pixels to pure white\n",
    "    arr[near_white_mask] = [255, 255, 255]\n",
    "    \n",
    "    # Convert back to PIL image if needed\n",
    "    return arr\n",
    "def extract_words_with_tesseract(doc_path, dpi_value, preprocessing_functions, oem_value, psm_value, existing_df=None, is_there_a_watermark=False): # Tesseract OCR word Extraction\n",
    "    data = []\n",
    "    scale = 72 / dpi_value\n",
    "    # Open the PDF document\n",
    "    doc = fitz.open(doc_path)\n",
    "    num_pages = len(doc)\n",
    "    if existing_df is not None:\n",
    "        existing_words = existing_df[['Page', 'Text', 'x0', 'y0', 'x1', 'y1']].copy()\n",
    "    else:\n",
    "        existing_words = pd.DataFrame(columns=['Page', 'Text', 'x0', 'y0', 'x1', 'y1'])\n",
    "\n",
    "    for page_num in tqdm(range(num_pages), desc=\"Processing Pages\", unit=\"page\"):\n",
    "        page = doc[page_num]\n",
    "        page_height_points = page.rect.height\n",
    "        # Render page to an image (Pixmap)\n",
    "        pix = page.get_pixmap(dpi=dpi_value)\n",
    "        img_bytes = pix.tobytes()\n",
    "\n",
    "        # Open the image with PIL\n",
    "        image = Image.open(io.BytesIO(img_bytes))\n",
    "\n",
    "        # Remove watermark if necessary\n",
    "        if is_there_a_watermark:\n",
    "            # Convert image to bytes\n",
    "            img_byte_arr = io.BytesIO()\n",
    "            image.save(img_byte_arr, format='PNG')\n",
    "            image_bytes = img_byte_arr.getvalue()\n",
    "            # Remove watermark\n",
    "            processed_image_bytes = remove_watermark(image_bytes)\n",
    "            image = Image.open(io.BytesIO(processed_image_bytes))\n",
    "            \n",
    "        # Rotate image if necessary\n",
    "        if image.width > image.height:\n",
    "            # Rotate the image to portrait orientation\n",
    "            image = image.rotate(-90, expand=True)\n",
    "\n",
    "        # Loop through preprocessing functions\n",
    "        page_has_words = False\n",
    "        for round_name, preprocess_func in preprocessing_functions:\n",
    "            # Preprocess the image\n",
    "            preprocessed_image = preprocess_func(img_bytes)\n",
    "            pil_image = Image.fromarray(preprocessed_image)\n",
    "\n",
    "            # Define Tesseract config\n",
    "            custom_config = f'--oem {oem_value} --psm {psm_value}'\n",
    "\n",
    "            # Get per-word bounding box information\n",
    "            word_data = pytesseract.image_to_data(\n",
    "                pil_image, config=custom_config, output_type=pytesseract.Output.DICT\n",
    "            )\n",
    "\n",
    "            # Check if there are any recognized words on this page\n",
    "            # (Skip if no words or all words are empty strings)\n",
    "            if all(word.strip() == '' for word in word_data['text']):\n",
    "                # No words found by Tesseract for this preprocessing round\n",
    "                continue\n",
    "            else:\n",
    "                page_has_words = True\n",
    "            \n",
    "            # If words are found, process them\n",
    "            num_words = len(word_data['text'])\n",
    "            for i in range(num_words):\n",
    "                word = word_data['text'][i].strip()\n",
    "                conf = int(word_data['conf'][i])\n",
    "                if word and conf > 10:\n",
    "                    x0 = word_data['left'][i]\n",
    "                    y0 = word_data['top'][i]\n",
    "                    width = word_data['width'][i]\n",
    "                    height = word_data['height'][i]\n",
    "                    x1 = x0 + width\n",
    "                    y1 = y0 + height\n",
    "                    text_height = height * scale\n",
    "                    word_length = width * scale\n",
    "                    x0 = x0 * scale\n",
    "                    y0 = y0 * scale\n",
    "                    x1 = x1 * scale\n",
    "                    y1 = y1 * scale\n",
    "                    y0_inverted = page_height_points - y1\n",
    "                    y1_inverted = page_height_points - y0\n",
    "                    page_number = page_num + 1\n",
    "                    is_duplicate = False\n",
    "                    threshold = 3 * scale\n",
    "                    matching_words = existing_words[\n",
    "                        (existing_words['Page'] == page_number) &\n",
    "                        (existing_words['Text'] == word)\n",
    "                    ]\n",
    "                    for _, existing_word in matching_words.iterrows():\n",
    "                        dx = abs(existing_word['x0'] - x0)\n",
    "                        dy = abs(existing_word['y0'] - y0_inverted)\n",
    "                        if dx <= threshold and dy <= threshold:\n",
    "                            is_duplicate = True\n",
    "                            break\n",
    "                    if not is_duplicate:\n",
    "                        # Append the data to the list\n",
    "                        data.append({\n",
    "                            'Page': page_number,\n",
    "                            'Text': word,\n",
    "                            'x0': x0,\n",
    "                            'y0': y0_inverted,\n",
    "                            'x1': x1,\n",
    "                            'y1': y1_inverted,\n",
    "                            'textHeight': text_height,\n",
    "                            'wordLength': word_length,\n",
    "                            'Confidence': conf\n",
    "                        })\n",
    "                        new_row = pd.DataFrame([{\n",
    "                            'Page': page_number,\n",
    "                            'Text': word,\n",
    "                            'x0': x0,\n",
    "                            'y0': y0_inverted,\n",
    "                            'x1': x1,\n",
    "                            'y1': y1_inverted\n",
    "                        }])\n",
    "                        existing_words = pd.concat([existing_words, new_row], ignore_index=True)\n",
    "\n",
    "        # If after all preprocessing functions, no words were found, skip this page\n",
    "        if not page_has_words:\n",
    "            continue\n",
    "\n",
    "    # Create a pandas DataFrame\n",
    "    df = pd.DataFrame(data, columns=[\n",
    "        'Page', 'Text', 'x0', 'y0', 'x1', 'y1', 'textHeight', 'wordLength', 'Confidence'])\n",
    "\n",
    "    # Remove duplicates\n",
    "    if existing_df is not None:\n",
    "        combined_df = pd.concat([existing_df, df], ignore_index=True)\n",
    "        combined_df.drop_duplicates(subset=['Page', 'Text', 'x0', 'y0', 'x1', 'y1'], inplace=True)\n",
    "    else:\n",
    "        combined_df = df\n",
    "    return combined_df\n",
    "def flatten_pdf(input_pdf_path, output_pdf_path, dpi=300):\n",
    "    \"\"\"\n",
    "    Rasterizes each page of input_pdf_path into a PNG image, then rebuilds a PDF from these images.\n",
    "    This results in a flattened PDF with no vector/text layers, just images.\n",
    "    \"\"\"\n",
    "    temp_images = []\n",
    "    doc = fitz.open(input_pdf_path)\n",
    "    for page_num in range(len(doc)):\n",
    "        pix = doc[page_num].get_pixmap(dpi=dpi)\n",
    "        img_data = pix.tobytes(\"png\")\n",
    "        img_path = f\"flatten_temp_page_{page_num}.png\"\n",
    "        with open(img_path, \"wb\") as f:\n",
    "            f.write(img_data)\n",
    "        temp_images.append(img_path)\n",
    "    doc.close()\n",
    "\n",
    "    # Create a new PDF from the rendered images\n",
    "    img_doc = fitz.open()\n",
    "    for img_path in temp_images:\n",
    "        img = fitz.open(img_path)\n",
    "        pdf_bytes = img.convert_to_pdf()  # Convert image to a single-page PDF\n",
    "        img_pdf = fitz.open(\"pdf\", pdf_bytes)\n",
    "        img_doc.insert_pdf(img_pdf)\n",
    "        img.close()\n",
    "\n",
    "    img_doc.save(output_pdf_path)\n",
    "    img_doc.close()\n",
    "\n",
    "    # Clean up temporary image files\n",
    "    for img_path in temp_images:\n",
    "        os.remove(img_path)\n",
    "\n",
    "def tesseract_adjustment(PDFData, output_redacted_pdf_name, document_path, is_there_a_watermark, run_tesseract):\n",
    "    dpiValue = 400\n",
    "    oem_value = 3\n",
    "    psm_value = 6\n",
    "\n",
    "    # Extract text with PDFMiner\n",
    "    print(\"Extracting text with PDFMiner...\")\n",
    "    pdfminer_df = PDFData\n",
    "    document_dir = os.path.dirname(document_path)\n",
    "\n",
    "    # If run_tesseract is False, skip Tesseract and just redact PDFMiner results\n",
    "    if not run_tesseract:\n",
    "        print(\"Skipping Tesseract OCR since run_tesseract is False.\")\n",
    "\n",
    "        # Ensure 'Source' column exists\n",
    "        if 'Source' not in pdfminer_df.columns:\n",
    "            pdfminer_df['Source'] = 'PDFMiner'\n",
    "\n",
    "        # This will be our \"combined\" text since we have no Tesseract results\n",
    "        combined_text_df = pdfminer_df.sort_values(by=['Page', 'y0', 'x0'], ascending=[True, True, True])\n",
    "        final_redacted_pdf_path = os.path.join(document_dir, output_redacted_pdf_name)\n",
    "        redact_combined_text(document_path, combined_text_df, final_redacted_pdf_path, dpi_value=dpiValue)\n",
    "        print(f\"Final redacted PDF saved as '{final_redacted_pdf_path}'.\")\n",
    "        return combined_text_df\n",
    "\n",
    "    # If run_tesseract is True, continue with original logic:\n",
    "    if pdfminer_df.empty:\n",
    "        print(\"PDFMiner did not capture any text. Running Tesseract on the original PDF.\")\n",
    "        tesseract_input_path = document_path\n",
    "    else:\n",
    "        redacted_pdf_path = os.path.join(document_dir, 'pdfminer_redacted.pdf')\n",
    "        print(\"Redacting extracted text from PDFMiner results...\")\n",
    "        redact_combined_text(document_path, pdfminer_df, redacted_pdf_path, dpi_value=dpiValue)\n",
    "\n",
    "        # flattened_pdf_path = os.path.join(document_dir, 'pdfminer_redacted_flattened.pdf')\n",
    "        # print(\"Flattening redacted PDF...\")\n",
    "        # flatten_pdf(redacted_pdf_path, flattened_pdf_path, dpi = dpiValue)\n",
    "        tesseract_input_path = redacted_pdf_path\n",
    "\n",
    "    print(\"Extracting words with Tesseract...\")\n",
    "    preprocessing_functions = [\n",
    "        (\"white_cleaning\", make_near_white_pixels_white),\n",
    "        ('preprocess_image_1', preprocess_image_1),\n",
    "        ('preprocess_image_2', preprocess_image_2),\n",
    "        ('preprocess_image_3', preprocess_image_3),\n",
    "        ('preprocess_image_4', preprocess_image_4)\n",
    "    ]\n",
    "    start_time = time.time()\n",
    "    combined_df = extract_words_with_tesseract(\n",
    "        tesseract_input_path, dpiValue, preprocessing_functions, oem_value, psm_value, \n",
    "        existing_df=None, is_there_a_watermark=is_there_a_watermark\n",
    "    )\n",
    "\n",
    "    step_time = time.time() - start_time\n",
    "    print(f\"Tesseract OCR completed in {step_time:.2f} seconds\")\n",
    "\n",
    "    # Combine PDFMiner and Tesseract results\n",
    "    tesseract_df = combined_df[['Page', 'Text', 'x0', 'y0', 'x1', 'y1']]\n",
    "    tesseract_df['Source'] = 'Tesseract'\n",
    "    if 'Source' not in pdfminer_df.columns:\n",
    "        pdfminer_df['Source'] = 'PDFMiner'\n",
    "\n",
    "    combined_text_df = pd.concat([pdfminer_df, tesseract_df], ignore_index=True)\n",
    "    combined_text_df = combined_text_df.sort_values(by=['Page', 'y0', 'x0'], ascending=[True, True, True])\n",
    "    final_redacted_pdf_path = os.path.join(document_dir, output_redacted_pdf_name)\n",
    "    redact_combined_text(document_path, combined_text_df, final_redacted_pdf_path, dpi_value=dpiValue)\n",
    "\n",
    "    # Clean up intermediate file if created\n",
    "    if not pdfminer_df.empty:\n",
    "        redacted_pdf_path = os.path.join(document_dir, 'pdfminer_redacted.pdf')\n",
    "        if os.path.exists(redacted_pdf_path):\n",
    "            os.remove(redacted_pdf_path) \n",
    "            print(f\"Intermediate files '{redacted_pdf_path}' and has been deleted.\")\n",
    "\n",
    "    print(f\"Final redacted PDF saved as '{final_redacted_pdf_path}'.\")\n",
    "    return combined_text_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -- OCR CLEANUP --> Brut Text to Cropped & Clean lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reorder based on Y coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verticalOrdering(df):\n",
    "    # Convert 'y0' column to numeric values, errors='coerce' will replace non-numeric values with NaN\n",
    "    df['y0'] = pd.to_numeric(df['y0'], errors='coerce')\n",
    "    # Group by 'Page' and then apply sorting to each group\n",
    "    df_sorted = df.groupby('Page', group_keys=False).apply(lambda x: x.sort_values(by='y0', ascending=False))\n",
    "    return df_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Text on the Left and Right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeTextonTheLeftAndright(df, x0_Left, x0_Right):\n",
    "    # Remove rows starting before x0coord\n",
    "    filtered_df = df[(df['x0'] >= x0_Left) & (df['x0'] <= x0_Right)]\n",
    "    return pd.DataFrame(filtered_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Headers and Footers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_headers_and_footers_Manual(df, header_y0, footer_y1):\n",
    "    # Remove rows above the header_y0 value\n",
    "    df = df[df['y0'] <= header_y0]\n",
    "    # Remove rows below the footer_y1 value\n",
    "    df = df[df['y1'] >= footer_y1]\n",
    "    return pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gatherHeaderAndFooterText(df, page_height, h_area):\n",
    "    # Calculate header and footer limits\n",
    "    header_limit = (h_area / 100) * page_height\n",
    "    # Prepare lists to collect data for the output DataFrame\n",
    "    data = []\n",
    "    for page_number in df[\"Page\"].unique():\n",
    "        # Filter DataFrame for the current page\n",
    "        page_df = df[df[\"Page\"] == page_number]\n",
    "        # Separate header and footer areas\n",
    "        header_df = page_df[page_df[\"y0\"] >= (page_height - header_limit)]\n",
    "        footer_df = page_df[page_df[\"y1\"] <= header_limit]\n",
    "    \n",
    "        # Collect header text and coordinates\n",
    "        for _, row in header_df.iterrows():\n",
    "            data.append({\n",
    "                \"Page\": int(page_number),\n",
    "                \"Area\": \"Header\",\n",
    "                \"Text\": row[\"Text\"],\n",
    "                \"y0\": row[\"y0\"],\n",
    "                \"y1\": row[\"y1\"]\n",
    "            })\n",
    "\n",
    "        # Collect footer text and coordinates\n",
    "        for _, row in footer_df.iterrows():\n",
    "            data.append({\n",
    "                \"Page\": int(page_number),\n",
    "                \"Area\": \"Footer\",\n",
    "                \"Text\": row[\"Text\"],\n",
    "                \"y0\": row[\"y0\"],\n",
    "                \"y1\": row[\"y1\"]\n",
    "            })\n",
    "    # Convert the collected data into a DataFrame\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifyHeaderFooterText(df, similarity_threshold, pageCoverage): #Works but not if footer/header drastically different for a couple of pages\n",
    "    # Step 1: Get the total number of pages in the document\n",
    "    total_pages = df[\"Page\"].nunique()\n",
    "    # Step 2: Function to group similar texts using fuzzy matching\n",
    "    def group_similar_texts(area_df, similarity_threshold):\n",
    "        similarity_groups = []\n",
    "        group_map = {}\n",
    "\n",
    "        for idx, text in zip(area_df.index, area_df[\"Text\"]):\n",
    "            matched = False\n",
    "            for group_id, group_texts in enumerate(similarity_groups):\n",
    "                # Compare text with existing group representatives\n",
    "                if any(fuzz.ratio(text, existing) >= similarity_threshold for existing in group_texts):\n",
    "                    group_texts.append(text)\n",
    "                    group_map[idx] = group_id\n",
    "                    matched = True\n",
    "                    break\n",
    "            if not matched:\n",
    "                # Create a new group\n",
    "                similarity_groups.append([text])\n",
    "                group_map[idx] = len(similarity_groups) - 1\n",
    "\n",
    "        return group_map\n",
    "\n",
    "    # Step 3: Create header and footer group maps separately\n",
    "    headers = df[df[\"Area\"] == \"Header\"].copy()\n",
    "    footers = df[df[\"Area\"] == \"Footer\"].copy()\n",
    "\n",
    "    header_group_map = group_similar_texts(headers, similarity_threshold)\n",
    "    footer_group_map = group_similar_texts(footers, similarity_threshold)\n",
    "\n",
    "    # Assign similarity groups back to headers and footers\n",
    "    headers[\"Similarity Group\"] = headers.index.map(header_group_map)\n",
    "    footers[\"Similarity Group\"] = footers.index.map(footer_group_map)\n",
    "\n",
    "    # Combine headers, footers, and the rest of the data back into the main DataFrame\n",
    "    other_rows = df[~df[\"Area\"].isin([\"Header\", \"Footer\"])]\n",
    "    df = pd.concat([headers, footers, other_rows]).sort_index()\n",
    "\n",
    "    # Step 4: Count occurrences for headers and footers separately\n",
    "    header_occurrences = headers[\"Similarity Group\"].value_counts()\n",
    "    footer_occurrences = footers[\"Similarity Group\"].value_counts()\n",
    "\n",
    "    # Map occurrences back to the DataFrame\n",
    "    df[\"Header Group Occurrences\"] = df[\"Similarity Group\"].map(header_occurrences).fillna(0).astype(int)\n",
    "    df[\"Footer Group Occurrences\"] = df[\"Similarity Group\"].map(footer_occurrences).fillna(0).astype(int)\n",
    "\n",
    "    # Step 5: Calculate the occurrence ratio\n",
    "    df[\"Header Occurrence Ratio\"] = (df[\"Header Group Occurrences\"] / total_pages * 100).fillna(0)\n",
    "    df[\"Footer Occurrence Ratio\"] = (df[\"Footer Group Occurrences\"] / total_pages * 100).fillna(0)\n",
    "\n",
    "    # Step 6: Default Classification as 'Body'\n",
    "    df[\"Classification\"] = \"Body\"\n",
    "\n",
    "    # Step 7: Classify as Header or Footer based on the occurrence ratio\n",
    "    if total_pages > 100:\n",
    "        df.loc[(df[\"Area\"] == \"Header\") & (df[\"Header Occurrence Ratio\"] >= pageCoverage/20), \"Classification\"] = \"Header\"\n",
    "        df.loc[(df[\"Area\"] == \"Footer\") & (df[\"Footer Occurrence Ratio\"] >= pageCoverage/20), \"Classification\"] = \"Footer\"\n",
    "    else:\n",
    "        df.loc[(df[\"Area\"] == \"Header\") & (df[\"Header Group Occurrences\"] >= pageCoverage), \"Classification\"] = \"Header\"\n",
    "        df.loc[(df[\"Area\"] == \"Footer\") & (df[\"Footer Group Occurrences\"] >= pageCoverage), \"Classification\"] = \"Footer\"\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHeaderFooterLineCoordinates(df):\n",
    "    df[\"Header Line\"] = None\n",
    "    df[\"Footer Line\"] = None\n",
    "\n",
    "    # Group by page to process headers and footers for each page\n",
    "    for page, page_data in df.groupby(\"Page\"):\n",
    "        # Filter for headers and footers\n",
    "        header_rows = page_data[page_data[\"Classification\"] == \"Header\"]\n",
    "        footer_rows = page_data[page_data[\"Classification\"] == \"Footer\"]\n",
    "        \n",
    "        # Calculate header and footer line coordinates\n",
    "        header_line = header_rows[\"y0\"].min()-1 if not header_rows.empty else \"No Header\"\n",
    "        footer_line = footer_rows[\"y1\"].max()+1 if not footer_rows.empty else \"No Footer\"\n",
    "\n",
    "        # Update the original DataFrame with the calculated values\n",
    "        df.loc[df[\"Page\"] == page, \"Header Line\"] = header_line\n",
    "        df.loc[df[\"Page\"] == page, \"Footer Line\"] = footer_line\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def automaticallyRemoveHeadersFooter(File2, header_footer_lines):\n",
    "    # Create a copy of File2 to avoid modifying the original DataFrame\n",
    "    cleaned_df = File2.copy()\n",
    "\n",
    "    # Iterate through each page to apply header and footer line boundaries\n",
    "    for page in header_footer_lines[\"Page\"].unique():\n",
    "        # Get the header and footer line coordinates for the current page\n",
    "        page_lines = header_footer_lines[header_footer_lines[\"Page\"] == page]\n",
    "        header_line = page_lines.iloc[0][\"Header Line\"]\n",
    "        footer_line = page_lines.iloc[0][\"Footer Line\"]\n",
    "\n",
    "        # Remove rows in the header area\n",
    "        if header_line != \"No Header\":\n",
    "            cleaned_df = cleaned_df[\n",
    "                ~((cleaned_df[\"Page\"] == page) & (cleaned_df[\"y1\"] >= header_line))\n",
    "            ]\n",
    "\n",
    "        # Remove rows in the footer area\n",
    "        if footer_line != \"No Footer\":\n",
    "            cleaned_df = cleaned_df[\n",
    "                ~((cleaned_df[\"Page\"] == page) & (cleaned_df[\"y0\"] <= footer_line))\n",
    "            ]\n",
    "\n",
    "    # Return the cleaned DataFrame\n",
    "    return cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_headers_and_footers(document_path, File0, File2, header_y0, footer_y1, auto_header_footer, h_area):\n",
    "    doc = fitz.open(document_path)  # Open the document\n",
    "    page_height = doc[0].rect.height\n",
    "    page_count = len(doc)\n",
    "\n",
    "    if auto_header_footer:\n",
    "        # Automatic detection workflow\n",
    "        # Gather Header and Footer Text for all pages in a df \"page\"/\"Area\"/\"Text\"/\"y0\"/\"y1\"\n",
    "        header_footer_text = gatherHeaderAndFooterText(File2, page_height, h_area)\n",
    "        if header_footer_text.empty:\n",
    "            df = File2\n",
    "            header_footer_lines = []\n",
    "        else:\n",
    "            # Classify Header/Footer Text to exclude body text\n",
    "            classified_text = classifyHeaderFooterText(header_footer_text, similarity_threshold=85, pageCoverage=2)\n",
    "            # Get Header and Footer Line Coordinates\n",
    "            header_footer_lines = getHeaderFooterLineCoordinates(classified_text)\n",
    "            # Remove headers and footers automatically based on classification\n",
    "            df = automaticallyRemoveHeadersFooter(File2, header_footer_lines)\n",
    "    else:\n",
    "        # Manual mode\n",
    "        # Remove headers and footers based on manual parameters\n",
    "        df = remove_headers_and_footers_Manual(File2, header_y0, footer_y1)\n",
    "\n",
    "        # Construct a DataFrame for header_footer_lines using the provided manual coordinates.\n",
    "        # If no header is intended, set it to \"No Header\", similarly for footer\n",
    "        header_line_val = header_y0 if header_y0 is not None else \"No Header\"\n",
    "        footer_line_val = footer_y1 if footer_y1 is not None else \"No Footer\"\n",
    "\n",
    "        # Create a DataFrame that mirrors the structure from the automatic case,\n",
    "        # applying the same header/footer line to all pages.\n",
    "        header_footer_lines = pd.DataFrame({\n",
    "            \"Page\": range(1, page_count + 1),\n",
    "            \"Header Line\": [header_line_val] * page_count,\n",
    "            \"Footer Line\": [footer_line_val] * page_count\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(df), pd.DataFrame(header_footer_lines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## merge Items with similar y0 and rebuild lines as needed based on x0 coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_items_with_similar_y0(df, tolerance):\n",
    "    # Group by 'Page' and then apply sorting to each group\n",
    "    df_sorted = df.groupby('Page', group_keys=False).apply(lambda x: x.sort_values(by='y0', ascending=False))\n",
    "    \n",
    "    # Initialize a list to store the merged rows\n",
    "    merged_rows = []\n",
    "    \n",
    "    # Iterate through each page group\n",
    "    for page, group in df_sorted.groupby('Page'):\n",
    "        i = 0\n",
    "        while i < len(group):\n",
    "            current_row = group.iloc[i].copy()\n",
    "            temp_rows = [current_row]\n",
    "\n",
    "            while i + 1 < len(group) and abs(current_row['y0'] - group.iloc[i + 1]['y0']) < tolerance:\n",
    "                next_row = group.iloc[i + 1]\n",
    "                temp_rows.append(next_row)\n",
    "                i += 1\n",
    "\n",
    "            if len(temp_rows) > 1:\n",
    "                temp_rows = sorted(temp_rows, key=lambda row: row['x0'])\n",
    "\n",
    "            merged_row = temp_rows[0].copy()\n",
    "            for row in temp_rows[1:]:\n",
    "                merged_row['Text'] += ' ' + row['Text']\n",
    "                merged_row['x0'] = min(merged_row['x0'], row['x0'])\n",
    "                merged_row['x1'] = max(merged_row['x1'], row['x1'])\n",
    "                merged_row['y0'] = min(merged_row['y0'], row['y0'])\n",
    "                merged_row['y1'] = max(merged_row['y1'], row['y1'])\n",
    "\n",
    "            merged_rows.append(merged_row)\n",
    "            i += 1\n",
    "    \n",
    "    # Create a new DataFrame from the merged rows\n",
    "    df_merged = pd.DataFrame(merged_rows)\n",
    "    \n",
    "    # Recalculate y_gap\n",
    "    df_merged['y0'] = pd.to_numeric(df_merged['y0'], errors='coerce')\n",
    "    df_merged['y_gap'] = df_merged['y0'].diff().fillna(0).abs().round(2)\n",
    "\n",
    "    return df_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -- RECREATE PARAGRAPHS --> Clean Lines to Clean Paragraphs with artificial headings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate line gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_line_gap(df, margin, percentage_linegaps):\n",
    "    # Drop rows with NaN values in the 'y_gap' column\n",
    "    df = df.dropna(subset=['y_gap'])\n",
    "    # Check if there are any valid 'y_gap' values\n",
    "    if df.empty:\n",
    "        raise ValueError(\"No valid 'y_gap' values found in the DataFrame.\")\n",
    "    \n",
    "    # Count the occurrences of each 'y_gap' value\n",
    "    counts = df['y_gap'].value_counts()\n",
    "    \n",
    "    # Calculate the total count of unique 'y_gap' values\n",
    "    total_unique_count = counts.size\n",
    "    \n",
    "    # Calculate the cumulative percentage\n",
    "    counts_with_percentage = pd.DataFrame({'Count': counts})\n",
    "    counts_with_percentage['Percentage'] = (counts_with_percentage['Count'] / counts_with_percentage['Count'].sum()) * 100\n",
    "    counts_with_percentage['CumulativePercentage'] = counts_with_percentage['Percentage'].cumsum()\n",
    "    # Get the top % most common values\n",
    "    top_percent = counts_with_percentage[counts_with_percentage['CumulativePercentage'] < percentage_linegaps]\n",
    "    # Determine the highest value among the top 35% most common values\n",
    "    if pd.isna(top_percent.index.max()):\n",
    "        line_gap = counts.idxmax() + margin # Update with the most common value\n",
    "    else:\n",
    "        line_gap = top_percent.index.max() + margin\n",
    "\n",
    "\n",
    "    return line_gap, counts_with_percentage, total_unique_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge lines together into paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_lines(df, line_gap):#base function - Does not include paragrph gap = line gap with pattern detection\n",
    "    merged_data = []\n",
    "    previous_row = None\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        if previous_row is not None and row['y_gap'] <= line_gap:\n",
    "            previous_row['Text'] += ' ' + row['Text']\n",
    "            # Update the coordinates\n",
    "            previous_row['x0'] = min(previous_row['x0'], row['x0'])\n",
    "            previous_row['x1'] = max(previous_row['x1'], row['x1'])\n",
    "            previous_row['y0'] = min(previous_row['y0'], row['y0'])\n",
    "            previous_row['y1'] = max(previous_row['y1'], row['y1'])\n",
    "\n",
    "        else:\n",
    "            if previous_row is not None:\n",
    "                merged_data.append(previous_row)\n",
    "            previous_row = row.copy()\n",
    "    \n",
    "    if previous_row is not None:\n",
    "        merged_data.append(previous_row)   \n",
    "    return pd.DataFrame(merged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_linesWithPattern(df, line_gap, patterns, bullet_patterns, table_figure_patterns):#keeps lines separated if pattern is matched while paragrph gap = line gap\n",
    "    merged_data = []\n",
    "    previous_row = None \n",
    "    for index, row in df.iterrows():\n",
    "        # Check if the row is within the vertical proximity (y_gap)\n",
    "        if previous_row is not None and row['y_gap'] <= line_gap:\n",
    "            # Now check for pattern matches only within the vertical proximity\n",
    "            matches_patterns = any(re.search(pattern, row['Text']) for pattern in patterns)\n",
    "            matches_bullet_patterns = any(re.search(pattern, row['Text']) for pattern in bullet_patterns)\n",
    "            matches_table_figure_patterns = any(re.search(pattern, row['Text']) for pattern in table_figure_patterns)\n",
    "            if not matches_patterns and not matches_bullet_patterns and not matches_table_figure_patterns:\n",
    "                # If no pattern matches, merge the lines\n",
    "                previous_row['Text'] += ' ' + row['Text']\n",
    "                # Update the coordinates\n",
    "                previous_row['x0'] = min(previous_row['x0'], row['x0'])\n",
    "                previous_row['x1'] = max(previous_row['x1'], row['x1'])\n",
    "                previous_row['y0'] = min(previous_row['y0'], row['y0'])\n",
    "                previous_row['y1'] = max(previous_row['y1'], row['y1'])\n",
    "            else:\n",
    "                # If a pattern matches, don't merge and treat the row as a new entry\n",
    "                #print(f\"Skipping merge for row {index} due to pattern match: {row['Text']}\")\n",
    "                merged_data.append(previous_row)\n",
    "                previous_row = row.copy()\n",
    "        else:\n",
    "            # If not within vertical proximity, consider it a new line\n",
    "            if previous_row is not None:\n",
    "                merged_data.append(previous_row)\n",
    "            previous_row = row.copy()\n",
    "    if previous_row is not None:\n",
    "        merged_data.append(previous_row)\n",
    "    return pd.DataFrame(merged_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge_paragraphs_with_page_break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_paragraphs_with_page_break(df):\n",
    "    # Create a list to store the rows\n",
    "    rows = []\n",
    "    # Iterate through the dataframe by index\n",
    "    i = 0\n",
    "    while i < len(df) - 1:\n",
    "        current_row = df.iloc[i].copy()\n",
    "        next_row = df.iloc[i + 1]\n",
    "        # Check if the current row is not on the same page as the previous row\n",
    "        if current_row['Page'] != next_row['Page']:\n",
    "            # Define your condition for merging here\n",
    "            if (not current_row['Text'].endswith('.') and \n",
    "                not current_row['Text'].endswith(':') and\n",
    "                not current_row['Text'].endswith(';') and\n",
    "                #not current_row['Text'].endswith('END OF SECTION') and\n",
    "                #not next_row['Text'][0].isupper() and\n",
    "                not any(re.match(pattern, next_row['Text']) for pattern in patterns) and\n",
    "                not any(re.match(pattern, next_row['Text']) for pattern in bullet_patterns) and\n",
    "                not any(re.match(pattern, next_row['Text']) for pattern in table_figure_patterns)):\n",
    "                # Merge the 'Text' from the current and next rows\n",
    "                merged_text = current_row['Text'] + ' ' + next_row['Text']\n",
    "                # Update the 'Text' in the current row\n",
    "                current_row['Text'] = merged_text\n",
    "                # Update the coordinates\n",
    "                current_row['x0'] = min(current_row['x0'], next_row['x0'])\n",
    "                current_row['x1'] = max(current_row['x1'], next_row['x1'])\n",
    "                current_row['y0'] = min(current_row['y0'], next_row['y0'])\n",
    "                current_row['y1'] = max(current_row['y1'], next_row['y1'])\n",
    "                # Add the merged row to the list\n",
    "                rows.append(current_row)\n",
    "                # Skip the next row as it's been merged\n",
    "                i += 2\n",
    "                continue\n",
    "        # Add the current row to the list\n",
    "        rows.append(current_row)\n",
    "        i += 1\n",
    "    # Add the last row of the dataframe to the list if it wasn't merged\n",
    "    if i == len(df) - 1:\n",
    "        rows.append(df.iloc[-1])\n",
    "    # Create a new DataFrame from the list of rows\n",
    "    merged_df = pd.DataFrame(rows)\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add P[x], B[y], Table/Figure[z]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_Artificial_Headers(df, patterns, bullet_patterns, table_figure_patterns):\n",
    "    p_counter = 1\n",
    "    b_counter = 1\n",
    "    \n",
    "    def normalize_text(text):\n",
    "        # Remove all leading whitespace and replace multiple spaces with a single space\n",
    "        return re.sub(r'\\s+', ' ', text.lstrip())\n",
    "\n",
    "    def check_patterns(text):\n",
    "        nonlocal p_counter, b_counter\n",
    "        normalized_text = normalize_text(text)\n",
    "        \n",
    "        if any(re.match(pattern, normalized_text) for pattern in bullet_patterns):\n",
    "            result = f\"B[{b_counter}] {text}\"\n",
    "            b_counter += 1\n",
    "            # p_counter = 1  # Reset P counter\n",
    "            return result\n",
    "        elif any(re.match(pattern, normalized_text) for pattern in table_figure_patterns):\n",
    "            return f\"[Table/Figure] {text}\"\n",
    "        elif any(re.match(pattern, normalized_text) for pattern in patterns):\n",
    "            p_counter = 1  # Reset P counter\n",
    "            b_counter = 1  # Reset B counter\n",
    "            return text\n",
    "        else:\n",
    "            result = f\"P[{p_counter}] {text}\"\n",
    "            p_counter += 1\n",
    "            return result\n",
    "\n",
    "    new_df = df.copy()\n",
    "    new_df['Text'] = new_df['Text'].apply(check_patterns)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -- RVTM FORMATTING --> Clean Paragraphs to Section and Primary field of RVTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Section and Primary Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_headers_to_section(df, patterns):\n",
    "    # Create an empty 'Section' column\n",
    "    df['Section'] = ''\n",
    "    \n",
    "    # Iterate over each pattern in the patterns list\n",
    "    for pattern in patterns:\n",
    "        # Apply the pattern to create the 'Section' column using case-insensitive matching\n",
    "        df['Section'] = df.apply(lambda row: re.search(pattern, row['Text'], re.IGNORECASE).group() if re.search(pattern, row['Text'], re.IGNORECASE) else row['Section'], axis=1)\n",
    "        # Remove the matched pattern from 'Text' using case-insensitive matching\n",
    "        df['Text'] = df.apply(lambda row: re.sub(pattern, '', row['Text'], flags=re.IGNORECASE) if re.search(pattern, row['Text'], re.IGNORECASE) else row['Text'], axis=1)\n",
    "    \n",
    "    # Reorder columns to place 'Section' before 'Text'\n",
    "    cols = df.columns.tolist()\n",
    "    text_index = cols.index('Text')\n",
    "    cols.insert(text_index, cols.pop(cols.index('Section')))\n",
    "    df = df[cols]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Section Type column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sectionType(df, patterns):\n",
    "    df.insert(1, 'Section Type', '')\n",
    "    for i, row in df.iterrows():\n",
    "        section_text = row['Section']\n",
    "        for pattern in patterns:\n",
    "            if re.match(pattern, section_text):\n",
    "                df.at[i, 'Section Type'] = pattern\n",
    "                break\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Section Hiearchy Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:9: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:10: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:11: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:12: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:13: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:14: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:42: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:43: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:58: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:60: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:90: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:91: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:92: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:9: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:10: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:11: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:12: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:13: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:14: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:42: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:43: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:58: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:60: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:90: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:91: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:92: SyntaxWarning: invalid escape sequence '\\['\n",
      "C:\\Users\\Jacqueline.chen\\AppData\\Local\\Temp\\ipykernel_48504\\1063844643.py:9: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  RomanStyle1 = \"^\\s*(I|II|III|IV|V|VI|VII|VIII|IX|X|XI|XII|XIII|XIV|XV|XVI|XVII|XVIII|XIX|XX)\\.\\s*\"\n",
      "C:\\Users\\Jacqueline.chen\\AppData\\Local\\Temp\\ipykernel_48504\\1063844643.py:10: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  RomanStyle2 = \"^\\s*(I|II|III|IV|V|VI|VII|VIII|IX|X|XI|XII|XIII|XIV|XV|XVI|XVII|XVIII|XIX|XX)\\)\\s*\"\n",
      "C:\\Users\\Jacqueline.chen\\AppData\\Local\\Temp\\ipykernel_48504\\1063844643.py:11: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  RomanStyle3 = \"^\\s*\\((I|II|III|IV|V|VI|VII|VIII|IX|X|XI|XII|XIII|XIV|XV|XVI|XVII|XVIII|XIX|XX)\\)\\s*\"\n",
      "C:\\Users\\Jacqueline.chen\\AppData\\Local\\Temp\\ipykernel_48504\\1063844643.py:12: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  RomanStyle4 = \"^\\s*(i|ii|iii|iv|v|vi|vii|viii|ix|x|xi|xii|xiii|xiv|xv|xvi|xvii|xviii|xix|xx)\\.\\s*\"\n",
      "C:\\Users\\Jacqueline.chen\\AppData\\Local\\Temp\\ipykernel_48504\\1063844643.py:13: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  RomanStyle5 = \"^\\s*(i|ii|iii|iv|v|vi|vii|viii|ix|x|xi|xii|xiii|xiv|xv|xvi|xvii|xviii|xix|xx)\\)\\s*\"\n",
      "C:\\Users\\Jacqueline.chen\\AppData\\Local\\Temp\\ipykernel_48504\\1063844643.py:14: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  RomanStyle6 = \"^\\s*\\((i|ii|iii|iv|v|vi|vii|viii|ix|x|xi|xii|xiii|xiv|xv|xvi|xvii|xviii|xix|xx)\\)\\s*\"\n",
      "C:\\Users\\Jacqueline.chen\\AppData\\Local\\Temp\\ipykernel_48504\\1063844643.py:42: SyntaxWarning: invalid escape sequence '\\['\n",
      "  if ((str(section_type) == \"^P\\[\\d+\\]\\s\") or #and (str(previous_section_type) != \"^P\\[\\d+\\]\\s\" and str(previous_section_type) != \"^B\\[\\d+\\]\\s\") or\n",
      "C:\\Users\\Jacqueline.chen\\AppData\\Local\\Temp\\ipykernel_48504\\1063844643.py:43: SyntaxWarning: invalid escape sequence '\\['\n",
      "  (str(section_type) == \"^B\\[\\d+\\]\\s\") #and (str(previous_section_type) != \"^P\\[\\d+\\]\\s\" and str(previous_section_type) != \"^B\\[\\d+\\]\\s\")\n",
      "C:\\Users\\Jacqueline.chen\\AppData\\Local\\Temp\\ipykernel_48504\\1063844643.py:58: SyntaxWarning: invalid escape sequence '\\['\n",
      "  if str(section_type) == \"^P\\[\\d+\\]\\s\":\n",
      "C:\\Users\\Jacqueline.chen\\AppData\\Local\\Temp\\ipykernel_48504\\1063844643.py:60: SyntaxWarning: invalid escape sequence '\\['\n",
      "  if str(section_type) == \"^B\\[\\d+\\]\\s\":\n",
      "C:\\Users\\Jacqueline.chen\\AppData\\Local\\Temp\\ipykernel_48504\\1063844643.py:90: SyntaxWarning: invalid escape sequence '\\['\n",
      "  if (str(previous_section_type) == \"^P\\[\\d+\\]\\s\" and not previous_text.endswith(\":\") or\n",
      "C:\\Users\\Jacqueline.chen\\AppData\\Local\\Temp\\ipykernel_48504\\1063844643.py:91: SyntaxWarning: invalid escape sequence '\\['\n",
      "  str(previous_section_type) == \"^B\\[\\d+\\]\\s\" or #and not previous_text.endswith(\":\") or\n",
      "C:\\Users\\Jacqueline.chen\\AppData\\Local\\Temp\\ipykernel_48504\\1063844643.py:92: SyntaxWarning: invalid escape sequence '\\['\n",
      "  str(previous_section_type) == \"^\\[Table\\/Figure\\]\\s\"\n"
     ]
    }
   ],
   "source": [
    "def sectionHierarchy(df): ### V1 Version Safe to use utill V2 is adjusted\n",
    "    df.insert(2, 'Section Hierarchy', '')  # Insert a new column for Section Hierarchy\n",
    "    df.insert(1, 'Dict', '')  # Insert a new column for Dictionnary\n",
    "    hierarchy_dict = {}\n",
    "    last_type = None\n",
    "    last_hierarchy = None\n",
    "\n",
    "    # Section types Roman\n",
    "    RomanStyle1 = \"^\\s*(I|II|III|IV|V|VI|VII|VIII|IX|X|XI|XII|XIII|XIV|XV|XVI|XVII|XVIII|XIX|XX)\\.\\s*\"\n",
    "    RomanStyle2 = \"^\\s*(I|II|III|IV|V|VI|VII|VIII|IX|X|XI|XII|XIII|XIV|XV|XVI|XVII|XVIII|XIX|XX)\\)\\s*\"\n",
    "    RomanStyle3 = \"^\\s*\\((I|II|III|IV|V|VI|VII|VIII|IX|X|XI|XII|XIII|XIV|XV|XVI|XVII|XVIII|XIX|XX)\\)\\s*\"\n",
    "    RomanStyle4 = \"^\\s*(i|ii|iii|iv|v|vi|vii|viii|ix|x|xi|xii|xiii|xiv|xv|xvi|xvii|xviii|xix|xx)\\.\\s*\"\n",
    "    RomanStyle5 = \"^\\s*(i|ii|iii|iv|v|vi|vii|viii|ix|x|xi|xii|xiii|xiv|xv|xvi|xvii|xviii|xix|xx)\\)\\s*\"\n",
    "    RomanStyle6 = \"^\\s*\\((i|ii|iii|iv|v|vi|vii|viii|ix|x|xi|xii|xiii|xiv|xv|xvi|xvii|xviii|xix|xx)\\)\\s*\"\n",
    "    \n",
    "    # Section Types Non Roman\n",
    "    NonRomanStyle1 = r\"^(?!I\\.)[A-RTUWYZ]\\.\\s\"   \n",
    "    NonRomanStyle2 = r\"^([A-RTUWYZ])\\)\\s\"    \n",
    "    NonRomanStyle3 = r\"^\\(\\s*([A-RTUWYZ])\\s*\\)\"\n",
    "    NonRomanStyle4 = r\"^(?!i\\.)[a-rtuwyz]\\.\\s\"   \n",
    "    NonRomanStyle5 = r\"^([a-rtuwyz])\\)\\s\"      \n",
    "    NonRomanStyle6 = r\"^\\(\\s*([a-rtuwyz])\\s*\\)\"\n",
    "\n",
    "    margin = 10\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        section_type = row['Section Type']\n",
    "        current_x0 = row['x0']\n",
    "        current_section = row['Section']  # Get the Section value\n",
    "        \n",
    "        if index > 0:  # Ensure you're not accessing the previous row for the first row\n",
    "            previous_section_type = df.at[index - 1, 'Section Type']\n",
    "            previous_text = df.at[index - 1, 'Text']\n",
    "            previous_x0 = df.at[index - 1, 'x0']\n",
    "            previous_section = df.at[index - 1, 'Section']\n",
    "        \n",
    "        if section_type in hierarchy_dict: # Use the existing hierarchy if this type was seen before\n",
    "            \n",
    "            hierarchy, _ = hierarchy_dict[section_type]\n",
    "\n",
    "            ### Fix the paragraph and bullet increment issue where we had a higher hierarchy level between.\n",
    "            if ((str(section_type) == \"^P\\[\\d+\\]\\s\") or #and (str(previous_section_type) != \"^P\\[\\d+\\]\\s\" and str(previous_section_type) != \"^B\\[\\d+\\]\\s\") or\n",
    "                (str(section_type) == \"^B\\[\\d+\\]\\s\") #and (str(previous_section_type) != \"^P\\[\\d+\\]\\s\" and str(previous_section_type) != \"^B\\[\\d+\\]\\s\")\n",
    "                ):\n",
    "                # Gather previous section and extract \"N\" value\n",
    "                _ , previous_index = hierarchy_dict[section_type]\n",
    "                SectionValuePrevindex = df.at[previous_index, 'Section']\n",
    "                match = re.search(r\"[PB]\\[(\\d+)\\]\", SectionValuePrevindex)\n",
    "                n_value_previous = int(match.group(1))\n",
    "\n",
    "                # Extract the number N from current P[N] or B[N]\n",
    "                match = re.search(r\"[PB]\\[(\\d+)\\]\", current_section)\n",
    "                n_value = int(match.group(1))\n",
    "\n",
    "                # If current = previous add one and save result in section column\n",
    "                if n_value <= n_value_previous:\n",
    "                    new_n_value = n_value_previous + 1\n",
    "                    if str(section_type) == \"^P\\[\\d+\\]\\s\":\n",
    "                        NewSection = f\"P[{new_n_value}]\"\n",
    "                    if str(section_type) == \"^B\\[\\d+\\]\\s\":\n",
    "                        NewSection = f\"B[{new_n_value}]\"\n",
    "                    df.at[index, 'Section'] = NewSection\n",
    "                    # Update the dictionary with the current index\n",
    "                    hierarchy_dict[section_type] = (hierarchy, index)\n",
    "                    # print(f\"Current Index : {index}\")\n",
    "                    # print(f\"Previous Index : {previous_index}\")\n",
    "                    # print(f\"Previous Section : {SectionValuePrevindex}\")\n",
    "                    # print(f\"new section : {NewSection}\")\n",
    "            ### END Fix the paragraph and bullet increment issue where we had a higher hierarchy level between\n",
    "            else:\n",
    "                #Update Dictionary Section value with most up to date\n",
    "                del hierarchy_dict[section_type] # Remove current with old data\n",
    "                hierarchy_dict[section_type] = (hierarchy, index) #add current with fresh data             \n",
    "\n",
    "        else: # Set hierarchy for a new type\n",
    "            if last_type is None:\n",
    "                hierarchy = 0  # First type gets hierarchy 0\n",
    "                #hierarchy_dict[section_type] = hierarchy\n",
    "                hierarchy_dict[section_type] = (hierarchy, index)\n",
    "                #df.at[index, 'Dict'] = hierarchy_dict # Update \"Dict\" column for troubleshooting only\n",
    "\n",
    "            else:# If this section is a different type and we already have a hierarchy for the last one\n",
    "                ### Base case : We increase Hierarchy by one and add the new type with their new hierarchy to the dictionnary\n",
    "                hierarchy = last_hierarchy + 1\n",
    "                #hierarchy_dict[section_type] = hierarchy\n",
    "                hierarchy_dict[section_type] = (hierarchy, index)\n",
    "                #df.at[index, 'Dict'] = hierarchy_dict # Update \"Dict\" column for troubleshooting only\n",
    "                \n",
    "                #### Paragraph P[N] issue where the paragraph does not have a proper section to ensure it is not taken in account for\n",
    "                if (str(previous_section_type) == \"^P\\[\\d+\\]\\s\" and not previous_text.endswith(\":\") or\n",
    "                    str(previous_section_type) == \"^B\\[\\d+\\]\\s\" or #and not previous_text.endswith(\":\") or\n",
    "                    str(previous_section_type) == \"^\\[Table\\/Figure\\]\\s\"\n",
    "                ):\n",
    "                    #Get the last key in the dictionary\n",
    "                    last_key = list(hierarchy_dict.keys())[-1] \n",
    "                    last_item = hierarchy_dict.pop(last_key) # Remove last item that was added in the else statement above\n",
    "                    last_key = list(hierarchy_dict.keys())[-1]\n",
    "                    last_item = hierarchy_dict.pop(last_key) # Remove previous P[N] item \n",
    "                    hierarchy = last_hierarchy #update Hierarchy to previous\n",
    "                    #hierarchy_dict[section_type] = hierarchy # add current element to the list so hierarchy and types are updated moving forward\n",
    "                    hierarchy_dict[section_type] = (hierarchy, index)\n",
    "                    \n",
    "\n",
    "                ### Logic to deal with some of Roman Numeral Numbers issues\n",
    "                section_type = str(section_type)\n",
    "                previous_section_type = str(previous_section_type)\n",
    "                \n",
    "                if (RomanStyle1 == section_type and NonRomanStyle1 == previous_section_type or\n",
    "                    RomanStyle2 == section_type and NonRomanStyle2 == previous_section_type or\n",
    "                    RomanStyle3 == section_type and NonRomanStyle3 == previous_section_type or\n",
    "                    RomanStyle4 == section_type and NonRomanStyle4 == previous_section_type or \n",
    "                    RomanStyle5 == section_type and NonRomanStyle5 == previous_section_type or\n",
    "                    RomanStyle6 == section_type and NonRomanStyle6 == previous_section_type\n",
    "                ):\n",
    "                    \n",
    "                    if abs(current_x0 - previous_x0) <= margin: # This is in case we have H then I but there was in indentation +> we ignore\n",
    "                        hierarchy = last_hierarchy\n",
    "                        section_type = previous_section_type\n",
    "                        \n",
    "                ### END OF ROMAN ENUMERAL FIX ###\n",
    "        # Update DataFrame\n",
    "        df.at[index, 'Section Hierarchy'] = hierarchy\n",
    "        df.at[index, 'Section Type'] = section_type\n",
    "\n",
    "        # If current hierarchy is lower than the previous one, clear out the dictionary\n",
    "        if last_hierarchy is not None and hierarchy < last_hierarchy:\n",
    "            keys_to_remove = [key for key, value in hierarchy_dict.items() if value[0] > hierarchy]\n",
    "            for key in keys_to_remove:\n",
    "                del hierarchy_dict[key]\n",
    "\n",
    "        # Update last_type and last_hierarchy for the next iteration\n",
    "        last_type = section_type\n",
    "        last_hierarchy = hierarchy\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:43: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:43: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:47: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:49: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:64: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:65: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:75: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:76: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:89: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:91: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:108: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:109: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:110: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:43: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:43: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:47: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:49: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:64: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:65: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:75: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:76: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:89: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:91: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:108: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:109: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:110: SyntaxWarning: invalid escape sequence '\\['\n",
      "C:\\Users\\Jacqueline.chen\\AppData\\Local\\Temp\\ipykernel_48504\\1392183902.py:43: SyntaxWarning: invalid escape sequence '\\['\n",
      "  match_found = section_type == \"^P\\[\\d+\\]\\s\" or section_type == \"^B\\[\\d+\\]\\s\"\n",
      "C:\\Users\\Jacqueline.chen\\AppData\\Local\\Temp\\ipykernel_48504\\1392183902.py:43: SyntaxWarning: invalid escape sequence '\\['\n",
      "  match_found = section_type == \"^P\\[\\d+\\]\\s\" or section_type == \"^B\\[\\d+\\]\\s\"\n",
      "C:\\Users\\Jacqueline.chen\\AppData\\Local\\Temp\\ipykernel_48504\\1392183902.py:47: SyntaxWarning: invalid escape sequence '\\['\n",
      "  if str(section_type) == \"^P\\[\\d+\\]\\s\":\n",
      "C:\\Users\\Jacqueline.chen\\AppData\\Local\\Temp\\ipykernel_48504\\1392183902.py:49: SyntaxWarning: invalid escape sequence '\\['\n",
      "  elif str(section_type) == \"^B\\[\\d+\\]\\s\":\n",
      "C:\\Users\\Jacqueline.chen\\AppData\\Local\\Temp\\ipykernel_48504\\1392183902.py:64: SyntaxWarning: invalid escape sequence '\\['\n",
      "  if ((str(section_type) == \"^P\\[\\d+\\]\\s\") or\n",
      "C:\\Users\\Jacqueline.chen\\AppData\\Local\\Temp\\ipykernel_48504\\1392183902.py:65: SyntaxWarning: invalid escape sequence '\\['\n",
      "  (str(section_type) == \"^B\\[\\d+\\]\\s\")):\n",
      "C:\\Users\\Jacqueline.chen\\AppData\\Local\\Temp\\ipykernel_48504\\1392183902.py:75: SyntaxWarning: invalid escape sequence '\\['\n",
      "  elif ((str(df.at[prev_idx, 'Section Type']) == \"^P\\[\\d+\\]\\s\" or\n",
      "C:\\Users\\Jacqueline.chen\\AppData\\Local\\Temp\\ipykernel_48504\\1392183902.py:76: SyntaxWarning: invalid escape sequence '\\['\n",
      "  str(df.at[prev_idx, 'Section Type']) == \"^B\\[\\d+\\]\\s\")):\n",
      "C:\\Users\\Jacqueline.chen\\AppData\\Local\\Temp\\ipykernel_48504\\1392183902.py:89: SyntaxWarning: invalid escape sequence '\\['\n",
      "  if str(section_type) == \"^P\\[\\d+\\]\\s\":\n",
      "C:\\Users\\Jacqueline.chen\\AppData\\Local\\Temp\\ipykernel_48504\\1392183902.py:91: SyntaxWarning: invalid escape sequence '\\['\n",
      "  elif str(section_type) == \"^B\\[\\d+\\]\\s\":\n",
      "C:\\Users\\Jacqueline.chen\\AppData\\Local\\Temp\\ipykernel_48504\\1392183902.py:108: SyntaxWarning: invalid escape sequence '\\['\n",
      "  if (str(previous_section_type) == \"^P\\[\\d+\\]\\s\" and not previous_text.endswith(\":\") or\n",
      "C:\\Users\\Jacqueline.chen\\AppData\\Local\\Temp\\ipykernel_48504\\1392183902.py:109: SyntaxWarning: invalid escape sequence '\\['\n",
      "  str(previous_section_type) == \"^B\\[\\d+\\]\\s\" or #and not previous_text.endswith(\":\") or\n",
      "C:\\Users\\Jacqueline.chen\\AppData\\Local\\Temp\\ipykernel_48504\\1392183902.py:110: SyntaxWarning: invalid escape sequence '\\['\n",
      "  str(previous_section_type) == \"^\\[Table\\/Figure\\]\\s\"\n"
     ]
    }
   ],
   "source": [
    "def sectionHierarchyToAdjust(df): #V2 Version to adjust\n",
    "    df.insert(2, 'Section Hierarchy', '')  # Insert a new column for Section Hierarchy\n",
    "    df.insert(1, 'Dict', '')  # Insert a new column for Dictionary\n",
    "    hierarchy_dict = {}\n",
    "    last_type = None\n",
    "    last_hierarchy = None\n",
    "\n",
    "    # Section types Roman\n",
    "    RomanStyle1 = r\"^\\s*(I|II|III|IV|V|VI|VII|VIII|IX|X|XI|XII|XIII|XIV|XV|XVI|XVII|XVIII|XIX|XX)\\.\\s*\"\n",
    "    RomanStyle2 = r\"^\\s*(I|II|III|IV|V|VI|VII|VIII|IX|X|XI|XII|XIII|XIV|XV|XVI|XVII|XVIII|XIX|XX)\\)\\s*\"\n",
    "    RomanStyle3 = r\"^\\s*\\((I|II|III|IV|V|VI|VII|VIII|IX|X|XI|XII|XIII|XIV|XV|XVI|XVII|XVIII|XIX|XX)\\)\\s*\"\n",
    "    RomanStyle4 = r\"^\\s*(i|ii|iii|iv|v|vi|vii|viii|ix|x|xi|xii|xiii|xiv|xv|xvi|xvii|xviii|xix|xx)\\.\\s*\"\n",
    "    RomanStyle5 = r\"^\\s*(i|ii|iii|iv|v|vi|vii|viii|ix|x|xi|xii|xiii|xiv|xv|xvi|xvii|xviii|xix|xx)\\)\\s*\"\n",
    "    RomanStyle6 = r\"^\\s*\\((i|ii|iii|iv|v|vi|vii|viii|ix|x|xi|xii|xiii|xiv|xv|xvi|xvii|xviii|xix|xx)\\)\\s*\"\n",
    "    \n",
    "    # Section Types Non Roman\n",
    "    NonRomanStyle1 = r\"^(?!I\\.)[A-RTUWYZ]\\.\\s\"   \n",
    "    NonRomanStyle2 = r\"^([A-RTUWYZ])\\)\\s\"    \n",
    "    NonRomanStyle3 = r\"^\\(\\s*([A-RTUWYZ])\\s*\\)\"\n",
    "    NonRomanStyle4 = r\"^(?!i\\.)[a-rtuwyz]\\.\\s\"   \n",
    "    NonRomanStyle5 = r\"^([a-rtuwyz])\\)\\s\"      \n",
    "    NonRomanStyle6 = r\"^\\(\\s*([a-rtuwyz])\\s*\\)\"\n",
    "\n",
    "    margin = 5\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        section_type = row['Section Type']\n",
    "        current_x0 = row['x0']\n",
    "        current_section = row['Section']  # Get the Section value\n",
    "        \n",
    "        if index > 0:\n",
    "            previous_section_type = df.at[index - 1, 'Section Type']\n",
    "            previous_text = df.at[index - 1, 'Text']\n",
    "            previous_x0 = df.at[index - 1, 'x0']\n",
    "            previous_section = df.at[index - 1, 'Section']\n",
    "        \n",
    "        \n",
    "        if section_type in hierarchy_dict: \n",
    "            _, previous_index = hierarchy_dict[section_type]\n",
    "            previous_x0_value = float(df.at[previous_index, 'x0'] or 0)\n",
    "            hierarchy, _ = hierarchy_dict[section_type]\n",
    "            previous_hierarchy = df.at[index - 1, 'Section Hierarchy']\n",
    "            match_found = section_type == \"^P\\[\\d+\\]\\s\" or section_type == \"^B\\[\\d+\\]\\s\"\n",
    "            if abs(current_x0 - previous_x0_value) >= margin and current_x0 >= previous_x0_value:\n",
    "                hierarchy = last_hierarchy + 1\n",
    "                hierarchy_dict[section_type] = (hierarchy, index)\n",
    "                if str(section_type) == \"^P\\[\\d+\\]\\s\":\n",
    "                     df.at[index, 'Section']= \"P[1]\"\n",
    "                elif str(section_type) == \"^B\\[\\d+\\]\\s\":\n",
    "                    df.at[index, 'Section']= \"B[1]\"\n",
    "            else:\n",
    "                if match_found:\n",
    "                    for prev_idx in range(index - 1, -1, -1):\n",
    "                        if df.at[prev_idx, 'Section Hierarchy'] == \"0\":\n",
    "                            break  # Stop the loop when Section Hierarchy is \"0\"\n",
    "                        if section_type in hierarchy_dict and abs(current_x0 - df.at[prev_idx, 'x0']) < 2:\n",
    "                            last_matching_hierarchy = df.at[prev_idx, 'Section Hierarchy']\n",
    "                            if last_matching_hierarchy < hierarchy:\n",
    "                                df.at[index, 'Section Hierarchy'] = last_matching_hierarchy\n",
    "                                hierarchy = last_matching_hierarchy\n",
    "                                hierarchy_dict[section_type] = (hierarchy, index)\n",
    "                                break\n",
    "            if previous_hierarchy >= hierarchy:\n",
    "                if ((str(section_type) == \"^P\\[\\d+\\]\\s\") or \n",
    "                    (str(section_type) == \"^B\\[\\d+\\]\\s\")):\n",
    "                    last_matching_hierarchy = None      \n",
    "                    prev_idx = None\n",
    "                    last_matching_index = None\n",
    "\n",
    "                    for prev_idx in range(index - 1, -1, -1):\n",
    "                        df.at[index, 'Section Hierarchy']=hierarchy\n",
    "                        last_matching_hierarchy = df.at[prev_idx, 'Section Hierarchy']\n",
    "                        if df.at[prev_idx, 'Section Hierarchy'] == \"0\": \n",
    "                            break\n",
    "                        elif ((str(df.at[prev_idx, 'Section Type']) == \"^P\\[\\d+\\]\\s\" or\n",
    "                            str(df.at[prev_idx, 'Section Type']) == \"^B\\[\\d+\\]\\s\")):\n",
    "                            #if last_matching_hierarchy == df.at[index, 'Section Hierarchy']: Condition does not work, don't know why\n",
    "                            if str(df.at[index, 'Section Hierarchy'])== str(last_matching_hierarchy):\n",
    "                                last_matching_index = prev_idx\n",
    "                                break\n",
    "                            \n",
    "                    if last_matching_index is not None:\n",
    "                        SectionValuePrevindex = df.at[last_matching_index, 'Section']\n",
    "                        match = re.search(r\"[PB]\\[(\\d+)\\]\", SectionValuePrevindex)\n",
    "                        n_value_previous = int(match.group(1)) if match else 0\n",
    "                        match = re.search(r\"[PB]\\[(\\d+)\\]\", current_section)\n",
    "                        n_value = int(match.group(1)) if match else 0\n",
    "                        new_n_value = n_value_previous + 1\n",
    "                        if str(section_type) == \"^P\\[\\d+\\]\\s\":\n",
    "                            NewSection = f\"P[{new_n_value}]\"\n",
    "                        elif str(section_type) == \"^B\\[\\d+\\]\\s\":\n",
    "                            NewSection = f\"B[{new_n_value}]\"\n",
    "                        df.at[index, 'Section'] = NewSection\n",
    "                        hierarchy_dict[section_type] = (hierarchy, index)\n",
    "        else:\n",
    "            if last_type is None:\n",
    "                hierarchy = 0\n",
    "                hierarchy_dict[section_type] = (hierarchy, index)\n",
    "            else: \n",
    "                #Need a condition here for B[N] to get reset to 1 if there is no B[N] in the dictionary \n",
    "                # Ex: \n",
    "                # P[1],B[1]\n",
    "                # P[1],B[2]\n",
    "                # P[2]\n",
    "                # P[2],B[3] this is wrong (should be P[2],B[1])\n",
    "                hierarchy = last_hierarchy + 1\n",
    "                hierarchy_dict[section_type] = (hierarchy, index)\n",
    "                if (str(previous_section_type) == \"^P\\[\\d+\\]\\s\" and not previous_text.endswith(\":\") or\n",
    "                    str(previous_section_type) == \"^B\\[\\d+\\]\\s\" or #and not previous_text.endswith(\":\") or\n",
    "                    str(previous_section_type) == \"^\\[Table\\/Figure\\]\\s\"\n",
    "                ):\n",
    "                    #Get the last key in the dictionary\n",
    "                    last_key = list(hierarchy_dict.keys())[-1] \n",
    "                    last_item = hierarchy_dict.pop(last_key) # Remove last item that was added in the else statement above\n",
    "                    last_key = list(hierarchy_dict.keys())[-1]\n",
    "                    last_item = hierarchy_dict.pop(last_key) # Remove previous P[N] item \n",
    "                    hierarchy = last_hierarchy #update Hierarchy to previous\n",
    "                    #hierarchy_dict[section_type] = hierarchy # add current element to the list so hierarchy and types are updated moving forward\n",
    "                    hierarchy_dict[section_type] = (hierarchy, index)\n",
    "\n",
    "        df.at[index, 'Section Hierarchy'] = hierarchy\n",
    "        df.at[index, 'Section Type'] = section_type\n",
    "                        ### Logic to deal with some of Roman Numeral Numbers issues\n",
    "        if (RomanStyle1 == section_type and NonRomanStyle1 == previous_section_type or\n",
    "            RomanStyle2 == section_type and NonRomanStyle2 == previous_section_type or\n",
    "            RomanStyle3 == section_type and NonRomanStyle3 == previous_section_type or\n",
    "            RomanStyle4 == section_type and NonRomanStyle4 == previous_section_type or \n",
    "            RomanStyle5 == section_type and NonRomanStyle5 == previous_section_type or\n",
    "            RomanStyle6 == section_type and NonRomanStyle6 == previous_section_type\n",
    "            ):\n",
    "                    \n",
    "            if abs(current_x0 - previous_x0) <= margin: # This is in case we have H then I but there was in indentation +> we ignore\n",
    "                hierarchy = last_hierarchy\n",
    "                section_type = previous_section_type\n",
    "                        \n",
    "                ### END OF ROMAN NUMERAL FIX ###\n",
    "        # Update DataFrame\n",
    "        df.at[index, 'Section Hierarchy'] = hierarchy\n",
    "        df.at[index, 'Section Type'] = section_type\n",
    "\n",
    "        # If current hierarchy is lower than the previous one, clear out the dictionary\n",
    "        if last_hierarchy is not None and hierarchy < last_hierarchy:\n",
    "            keys_to_remove = [key for key, value in hierarchy_dict.items() if value[0] > hierarchy]\n",
    "            for key in keys_to_remove:\n",
    "                del hierarchy_dict[key]\n",
    "\n",
    "        if last_hierarchy is not None and hierarchy < last_hierarchy:\n",
    "            keys_to_remove = [key for key, value in hierarchy_dict.items() if value[0] > hierarchy]\n",
    "            for key in keys_to_remove:\n",
    "                del hierarchy_dict[key]\n",
    "\n",
    "        last_type = section_type\n",
    "        last_hierarchy = hierarchy\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Combined Section Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_combined_section(df):\n",
    "    # Reset index to handle indexing cleanly\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    combined_sections = []\n",
    "        \n",
    "    for i in range(df.shape[0]):\n",
    "        current_hierarchy = df.loc[i, 'Section Hierarchy']\n",
    "        # Strip the section in case it has leading/trailing spaces\n",
    "        current_section = str(df.loc[i, 'Section']).strip()\n",
    "            \n",
    "        if current_hierarchy == 0:\n",
    "            # Top level just becomes itself\n",
    "            combined_section = current_section\n",
    "        else:\n",
    "            # Get the last combined section for reference\n",
    "            prev_combined = combined_sections[-1]\n",
    "            prev_hierarchy = df.loc[i-1, 'Section Hierarchy']\n",
    "\n",
    "            # Split the previous combined section by \", \"\n",
    "            prev_parts = prev_combined.split(\", \")\n",
    "\n",
    "            if current_hierarchy > prev_hierarchy:\n",
    "                # Add a new \"child\" section\n",
    "                combined_section = f\"{prev_combined}, {current_section}\"\n",
    "            elif current_hierarchy == prev_hierarchy:\n",
    "                # Replace the last section at this level\n",
    "                combined_section = \", \".join(prev_parts[:-1]) + f\", {current_section}\"\n",
    "            else:\n",
    "                # Move up the hierarchy and replace the section at that level\n",
    "                levels_to_keep = current_hierarchy\n",
    "                # Keep only the first `levels_to_keep` parts and add the new one\n",
    "                combined_section = \", \".join(prev_parts[:levels_to_keep]) + f\", {current_section}\"\n",
    "\n",
    "        # Just to ensure no accidental extra whitespace creeps in\n",
    "        combined_section = combined_section.strip()\n",
    "        combined_sections.append(combined_section)\n",
    "        \n",
    "    # Insert the new column right before \"Section\" or wherever you'd like\n",
    "    df.insert(df.columns.get_loc('Section'), 'Combined Section', combined_sections)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -- GENERATE OUTPUTS --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print Rectangles on PDF based on X,Y coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawDetectedtext(df, doc, page_height, color, width, source_condition=None, condition_color=None, problematic_pages=None):\n",
    "    # If problematic_pages not provided, default to pages with rotations\n",
    "    if problematic_pages is None:\n",
    "        problematic_pages = [page_num + 1 for page_num in range(len(doc)) if doc[page_num].rotation != 0]\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        page_number = int(row[\"Page\"]) - 1  # PyMuPDF uses zero-based page indexing\n",
    "        page = doc.load_page(page_number)\n",
    "        \n",
    "        # Extract coordinates\n",
    "        x0, y0, x1, y1 = row[\"x0\"], row[\"y0\"], row[\"x1\"], row[\"y1\"]\n",
    "\n",
    "        # Apply coordinate transformations:\n",
    "        # If the page is problematic, swap coordinates as done in redact_combined_text\n",
    "        if (page_number + 1) in problematic_pages:\n",
    "            rect = fitz.Rect(y0, x0, y1, x1)\n",
    "        else:\n",
    "            # Adjust the y-coordinates for PyMuPDF coordinate system on normal pages\n",
    "            y0_adjusted = page_height - y1\n",
    "            y1_adjusted = page_height - y0\n",
    "            rect = fitz.Rect(x0, y0_adjusted, x1, y1_adjusted)\n",
    "        \n",
    "        # Highlight or draw rectangle based on the source condition\n",
    "        if source_condition and row[\"Source\"] == source_condition:\n",
    "            # Highlight Tesseract text\n",
    "            page.add_highlight_annot(rect)\n",
    "        else:\n",
    "            # Draw a rectangle for other text\n",
    "            page.draw_rect(rect, color=color, width=width)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawHeaderFooterArea(doc, page_height, page_width, h_area, color, segment_length, width, spacing, problematic_pages=None):\n",
    "    # If problematic_pages not provided, default to pages with rotations\n",
    "    if problematic_pages is None:\n",
    "        problematic_pages = [page_num + 1 for page_num in range(len(doc)) if doc[page_num].rotation != 0]\n",
    "\n",
    "    # Calculate the y positions for the header and footer lines\n",
    "    header_y = (h_area / 100) * page_height\n",
    "    footer_y = page_height - header_y\n",
    "\n",
    "    for page_number in range(len(doc)):\n",
    "        page = doc.load_page(page_number)\n",
    "\n",
    "        # Check if this page is rotated\n",
    "        if (page_number + 1) in problematic_pages:\n",
    "            # For rotated pages, treat header_y and footer_y as x-coordinates for vertical lines\n",
    "            # Instead of drawing lines across the width at a certain y, \n",
    "            # we draw them across the height at a certain x.\n",
    "\n",
    "            # Draw vertical \"header\" segments\n",
    "            x_pos = 0\n",
    "            while x_pos < page_height:\n",
    "                # Here, we are swapping x and y usage to draw vertical lines instead of horizontal\n",
    "                page.draw_line(\n",
    "                    p1=(header_y, x_pos),\n",
    "                    p2=(header_y, x_pos + segment_length),\n",
    "                    color=color,\n",
    "                    width=width\n",
    "                )\n",
    "                x_pos += segment_length + spacing\n",
    "\n",
    "            # Draw vertical \"footer\" segments\n",
    "            x_pos = 0\n",
    "            while x_pos < page_height:\n",
    "                page.draw_line(\n",
    "                    p1=(footer_y, x_pos),\n",
    "                    p2=(footer_y, x_pos + segment_length),\n",
    "                    color=color,\n",
    "                    width=width\n",
    "                )\n",
    "                x_pos += segment_length + spacing\n",
    "\n",
    "        else:\n",
    "            # For non-rotated pages, draw horizontal lines as before.\n",
    "            # Draw header line segments\n",
    "            x_pos = 0\n",
    "            while x_pos < page_width:\n",
    "                page.draw_line(\n",
    "                    p1=(x_pos, header_y),\n",
    "                    p2=(x_pos + segment_length, header_y),\n",
    "                    color=color,\n",
    "                    width=width\n",
    "                )\n",
    "                x_pos += segment_length + spacing\n",
    "\n",
    "            # Draw footer line segments\n",
    "            x_pos = 0\n",
    "            while x_pos < page_width:\n",
    "                page.draw_line(\n",
    "                    p1=(x_pos, footer_y),\n",
    "                    p2=(x_pos + segment_length, footer_y),\n",
    "                    color=color,\n",
    "                    width=width\n",
    "                )\n",
    "                x_pos += segment_length + spacing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawHeaderFooterLines(df, doc, page_width, page_height, color, width, problematic_pages=None):\n",
    "    # If problematic_pages not provided, default to pages with rotations\n",
    "    if problematic_pages is None:\n",
    "        problematic_pages = [page_num + 1 for page_num in range(len(doc)) if doc[page_num].rotation != 0]\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        page_number = int(row[\"Page\"]) - 1  # zero-based index for PyMuPDF\n",
    "        header_line = row[\"Header Line\"]\n",
    "        footer_line = row[\"Footer Line\"]\n",
    "\n",
    "        page = doc.load_page(page_number)\n",
    "\n",
    "        # Adjust the y-coordinates to PyMuPDF's coordinate system if lines exist\n",
    "        header_line_adjusted = None if header_line == \"No Header\" else (page_height - header_line)\n",
    "        footer_line_adjusted = None if footer_line == \"No Footer\" else (page_height - footer_line)\n",
    "\n",
    "        # Check if this is a problematic (rotated) page\n",
    "        if (page_number + 1) in problematic_pages:\n",
    "            # On rotated pages, we treat the lines similarly to how text rectangles are treated:\n",
    "            # Swap the coordinate axes. A horizontal line at y would become a vertical line at x.\n",
    "            \n",
    "            # Draw the header line if it exists\n",
    "            if header_line_adjusted is not None:\n",
    "                # Instead of drawing horizontally across page_width, draw vertically across page_height\n",
    "                page.draw_line(\n",
    "                    p1=(header_line_adjusted, 0),\n",
    "                    p2=(header_line_adjusted, page_height),\n",
    "                    color=color,\n",
    "                    width=width\n",
    "                )\n",
    "\n",
    "            # Draw the footer line if it exists\n",
    "            if footer_line_adjusted is not None:\n",
    "                # Similarly, draw a vertical line for the footer\n",
    "                page.draw_line(\n",
    "                    p1=(footer_line_adjusted, 0),\n",
    "                    p2=(footer_line_adjusted, page_height),\n",
    "                    color=color,\n",
    "                    width=width\n",
    "                )\n",
    "        else:\n",
    "            # Non-rotated page: draw lines normally (horizontal)\n",
    "            if header_line_adjusted is not None:\n",
    "                page.draw_line(\n",
    "                    p1=(0, header_line_adjusted),\n",
    "                    p2=(page_width, header_line_adjusted),\n",
    "                    color=color,\n",
    "                    width=width\n",
    "                )\n",
    "\n",
    "            if footer_line_adjusted is not None:\n",
    "                page.draw_line(\n",
    "                    p1=(0, footer_line_adjusted),\n",
    "                    p2=(page_width, footer_line_adjusted),\n",
    "                    color=color,\n",
    "                    width=width\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printOnPDF(File2, File3, document_path, output_pdf_path, header_footer_lines, h_area):\n",
    "    doc = fitz.open(document_path)\n",
    "    page_height, page_width = doc[0].rect.height, doc[0].rect.width\n",
    "\n",
    "    # Draw Detected Text\n",
    "    drawDetectedtext(File2, doc, page_height, color=(1,0,0), width = 0.5, source_condition=\"Tesseract\", condition_color=(1, 0.75, 0.8)) # print lines and Tesseract text \n",
    "    drawDetectedtext(File3, doc, page_height, color=(0, 0, 1), width=1) # print paragraphs\n",
    "    # Draw Header and Footer Area\n",
    "    h_area = 15  # Defines Header/Footer area (percentage of page height)\n",
    "    drawHeaderFooterArea(doc, page_height, page_width, h_area, color=(0.5, 0.5, 0.5), segment_length=10, width=0.5, spacing=40)\n",
    "\n",
    "    # Draw Header and Footer Lines\n",
    "    drawHeaderFooterLines(header_footer_lines, doc, page_width, page_height, color=(0.75, 0, 0.75), width=1)\n",
    "    \n",
    "    # Save the annotated PDF\n",
    "    doc.save(output_pdf_path)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print DF to Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_missed_text_and_tables(document_path, dpi_value=300):\n",
    "    document_dir = os.path.dirname(document_path)\n",
    "    redacted_file_path = os.path.join(document_dir, \"final_redacted.pdf\")\n",
    "\n",
    "    # Check if the redacted file exists\n",
    "    if not os.path.isfile(redacted_file_path):\n",
    "        print(f\"Error: The file 'final_redacted.pdf' does not exist in {document_dir}.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    missed_data_details = []\n",
    "\n",
    "    try:\n",
    "        print(f\"Processing redacted document: {redacted_file_path}\")\n",
    "        doc = fitz.open(redacted_file_path)\n",
    "        num_pages = len(doc)\n",
    "\n",
    "        for page_num in range(num_pages):\n",
    "            page_index = page_num + 1\n",
    "            page = doc[page_num]\n",
    "\n",
    "            # Render page to image\n",
    "            pix = page.get_pixmap(dpi=dpi_value)\n",
    "            img_bytes = pix.tobytes(output='png')\n",
    "            img_array = np.frombuffer(img_bytes, np.uint8)\n",
    "            image = cv2.imdecode(img_array, cv2.IMREAD_COLOR)\n",
    "\n",
    "            # Preprocess image\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "            thresh = cv2.adaptiveThreshold(\n",
    "                blurred, 255, cv2.ADAPTIVE_THRESH_MEAN_C,\n",
    "                cv2.THRESH_BINARY_INV, 11, 4\n",
    "            )\n",
    "\n",
    "            # Remove horizontal lines\n",
    "            horizontal_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (40, 1))\n",
    "            remove_horizontal = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, horizontal_kernel, iterations=2)\n",
    "\n",
    "            # Remove vertical lines\n",
    "            vertical_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1, 40))\n",
    "            remove_vertical = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, vertical_kernel, iterations=2)\n",
    "\n",
    "            # Check for possible tables\n",
    "            has_horizontal_lines = np.count_nonzero(remove_horizontal) > 0\n",
    "            has_vertical_lines = np.count_nonzero(remove_vertical) > 0\n",
    "            has_table = has_horizontal_lines and has_vertical_lines\n",
    "\n",
    "            # Combine horizontal and vertical lines\n",
    "            lines = cv2.bitwise_or(remove_horizontal, remove_vertical)\n",
    "\n",
    "            # Subtract lines from thresholded image\n",
    "            detect_regions = cv2.subtract(thresh, lines)\n",
    "\n",
    "            # Find contours of the regions\n",
    "            contours, _ = cv2.findContours(detect_regions, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "            missed_areas_on_page = 0\n",
    "            for cnt in contours:\n",
    "                x, y, w, h = cv2.boundingRect(cnt)\n",
    "                # Filter out small regions that are likely noise\n",
    "                if w > 10 and h > 10:\n",
    "                    aspect_ratio = w / float(h)\n",
    "                    # Exclude horizontal lines and very elongated regions\n",
    "                    if aspect_ratio < 5:\n",
    "                        missed_areas_on_page += 1\n",
    "\n",
    "            # Only add rows for pages with mistakes or tables\n",
    "            if missed_areas_on_page > 0 or has_table:\n",
    "                missed_data_details.append({\n",
    "                    'Page': page_index,\n",
    "                    '# of Mistakes on Page': missed_areas_on_page,\n",
    "                    'Possible Table': 'Yes' if has_table else 'No',\n",
    "                })\n",
    "\n",
    "        print(f\"Finished processing redacted document: {redacted_file_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing document '{redacted_file_path}': {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Create a DataFrame with the results\n",
    "    missed_data_df = pd.DataFrame(missed_data_details)\n",
    "    return missed_data_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ToExcel(File, FileName, spell_checked_df=None, page_mistake_counts_df=None):\n",
    "    \"\"\"\n",
    "    Exports main data to Excel and optionally adds spell check and benchmark data as new sheets.\n",
    "    \n",
    "    Parameters:\n",
    "        File (list or DataFrame): Main data to export.\n",
    "        FileName (str): Path to save the Excel file.\n",
    "        spell_checked_df (DataFrame, optional): Spell check results to add as a new sheet.\n",
    "        page_mistake_counts_df (DataFrame, optional): Benchmark data to add as a new sheet.\n",
    "    \"\"\"\n",
    "    # Convert File to DataFrame\n",
    "    try:\n",
    "        Excel = pd.DataFrame(File)\n",
    "    except ValueError as e:\n",
    "        raise ValueError(f\"Error converting input data to DataFrame: {e}\")\n",
    "\n",
    "    # Desired final columns\n",
    "    final_columns = ['Page', 'Combined Section', 'Text']\n",
    "\n",
    "    # Reindex to ensure only the desired final columns exist.\n",
    "    # If a column does not exist, fill it with empty strings or NaN.\n",
    "    Excel = Excel.reindex(columns=final_columns)\n",
    "\n",
    "    # Write all data to the same Excel file using ExcelWriter\n",
    "    with pd.ExcelWriter(FileName, engine='openpyxl') as writer:\n",
    "        # Write Main Data\n",
    "        if not Excel.empty:\n",
    "            Excel.to_excel(writer, sheet_name='Main', index=False)\n",
    "        else:\n",
    "            print(\"Warning: Main data is empty. No data written to 'Main' sheet.\")\n",
    "\n",
    "        # Write Spell Check Results\n",
    "        if spell_checked_df is not None and not spell_checked_df.empty:\n",
    "            spell_checked_df.to_excel(writer, sheet_name='Spell Check', index=False)\n",
    "        else:\n",
    "            print(\"Warning: Spell check data is empty or None. No data written to 'Spell Check' sheet.\")\n",
    "\n",
    "        # Write Benchmark Data\n",
    "        if page_mistake_counts_df is not None and not page_mistake_counts_df.empty:\n",
    "            page_mistake_counts_df.to_excel(writer, sheet_name='Benchmark Pages with Mistakes', index=False)\n",
    "        else:\n",
    "            print(\"Warning: Benchmark data is empty or None. No data written to 'Benchmark Pages with Mistakes' sheet.\")\n",
    "\n",
    "    print(f\"Excel file created: '{FileName}'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPELLCHEKER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_word(word, spell):\n",
    "    # Try splitting the word into two words at every possible position\n",
    "    for i in range(1, len(word)):\n",
    "        first = word[:i]\n",
    "        second = word[i:]\n",
    "        if first.lower() in spell and second.lower() in spell:\n",
    "            return f\"{first} {second}\"\n",
    "    return None\n",
    "\n",
    "def perform_spellcheckPrev(combined_df):\n",
    "    spell = SpellChecker()\n",
    "    spell_checked_data = []\n",
    "    known_acronyms = {'ANSI', 'ACI', 'NSF', 'ASTM', 'ISO', 'IEC', 'IEEE', 'FDA', 'EPA', 'min.', 'submittals'} # Add more as needed\n",
    "    for index, row in tqdm(combined_df.iterrows(), total=combined_df.shape[0], desc=\"Spell Checking\", unit=\"word\"):\n",
    "        word = row['Text']\n",
    "        page_number = row['Page']\n",
    "        # Remove leading and trailing punctuation\n",
    "        word = word.strip(string.punctuation)\n",
    "        # Skip if word is empty after stripping punctuation\n",
    "        if not word:\n",
    "            continue\n",
    "        # Skip if word is a single character or numeric\n",
    "        if len(word) <= 1 or word.isnumeric():\n",
    "            continue\n",
    "        # Skip if word is a known acronym\n",
    "        if word.upper() in known_acronyms:\n",
    "            continue\n",
    "        # Skip if word is capitalized (proper noun) or all uppercase (acronym)\n",
    "        if word[0].isupper() or word.isupper():\n",
    "            continue\n",
    "        # Check if word is in the dictionary\n",
    "        if word.lower() in spell:\n",
    "            continue  # Word is correct\n",
    "        else:\n",
    "            # Get the most likely correction\n",
    "            correction = spell.correction(word)\n",
    "            # Try splitting the word\n",
    "            split_correction = split_word(word, spell)\n",
    "            if split_correction:\n",
    "                suggested_correction = split_correction\n",
    "            elif correction != word:\n",
    "                suggested_correction = correction\n",
    "            else:\n",
    "                suggested_correction = None\n",
    "            if suggested_correction and suggested_correction.lower() != word.lower():\n",
    "                spell_checked_data.append({\n",
    "                    'Page': page_number,\n",
    "                    'Text': row['Text'],  # Use the original text including punctuation\n",
    "                    'Suggested Correction': suggested_correction\n",
    "                })\n",
    "    # Create a dataframe with the spellchecked words\n",
    "    spell_checked_df = pd.DataFrame(spell_checked_data)\n",
    "    return spell_checked_df\n",
    "\n",
    "def perform_spellcheck(combined_df):\n",
    "    spell = SpellChecker()\n",
    "    spell_checked_data = []\n",
    "    known_acronyms = {'ANSI', 'ACI', 'NSF', 'ASTM', 'ISO', 'IEC', 'IEEE', 'FDA', 'EPA', 'min.', 'submittals'}  # Add more as needed\n",
    "\n",
    "    for index, row in tqdm(combined_df.iterrows(), total=combined_df.shape[0], desc=\"Spell Checking\", unit=\"word\"):\n",
    "        original_text = row['Text']  # Preserve original text\n",
    "\n",
    "        # Handle non-string values and skip NaN values\n",
    "        if not isinstance(original_text, str):\n",
    "            if pd.isna(original_text):  # Skip NaN values\n",
    "                continue\n",
    "            original_text = str(original_text)  # Convert non-string values to strings\n",
    "\n",
    "        word = original_text.strip(string.punctuation)  # Strip punctuation\n",
    "\n",
    "        # Skip irrelevant or correct words\n",
    "        if not word or len(word) <= 1 or word.isnumeric() or word.upper() in known_acronyms or word[0].isupper() or word.isupper():\n",
    "            continue\n",
    "\n",
    "        if word.lower() in spell:\n",
    "            continue  # Word is correct\n",
    "        else:\n",
    "            correction = spell.correction(word)\n",
    "            split_correction = split_word(word, spell)\n",
    "\n",
    "            if split_correction:\n",
    "                suggested_correction = split_correction\n",
    "            elif correction != word:\n",
    "                suggested_correction = correction\n",
    "            else:\n",
    "                suggested_correction = None\n",
    "\n",
    "            if suggested_correction and suggested_correction.lower() != word.lower():\n",
    "                spell_checked_data.append({\n",
    "                    'Page': row['Page'],\n",
    "                    'Text': original_text,  # Use preserved original text\n",
    "                    'Suggested Correction': suggested_correction\n",
    "                })\n",
    "\n",
    "    # Create a dataframe with the spellchecked words\n",
    "    spell_checked_df = pd.DataFrame(spell_checked_data)\n",
    "    return spell_checked_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -- MAIN --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runScript(document_path, output_excel, output_pdf_path, output_redacted_pdf_name, header_y0, footer_y1,\n",
    "              auto_header_footer, h_area, x0_Left, x0_Right, tolerence, line_gap_margin, percentage_linegaps,\n",
    "              mergeLineclassic, is_there_a_watermark, run_tesseract):\n",
    "    # Define global variables so we can debug using Jupyter Variables\n",
    "    global PdfData\n",
    "    global Tesseract_file\n",
    "    global File0   \n",
    "    global File1   \n",
    "    global File2   \n",
    "    global File3  \n",
    "    global File4\n",
    "    global File5\n",
    "    global File6\n",
    "    global File7\n",
    "    global File8\n",
    "    global File9\n",
    "\n",
    "    # Beginning of conversion\n",
    "    PdfData = CreateDataFrameFromPDF(document_path)\n",
    "\n",
    "    # Ensure the 'Source' column exists for PdfData\n",
    "    if 'Source' not in PdfData.columns:\n",
    "        PdfData['Source'] = 'PDFMiner'  # Set the default source as 'PDFMiner'\n",
    "\n",
    "    Tesseract_file = tesseract_adjustment(PdfData, output_redacted_pdf_name, document_path, is_there_a_watermark, run_tesseract)\n",
    "    if run_tesseract:\n",
    "        # Filter rows with 'Source' == 'Tesseract'\n",
    "        tesseract_only = Tesseract_file['Source'] == 'Tesseract'\n",
    "        # Perform spellcheck on the filtered DataFrame\n",
    "        spell_checked_df = perform_spellcheck(Tesseract_file.loc[tesseract_only, :])\n",
    "        \n",
    "        # Merge corrections back into Tesseract_file\n",
    "        for index, row in spell_checked_df.iterrows():\n",
    "            Tesseract_file.loc[index, 'Text'] = row['Suggested Correction'] if not pd.isna(row['Suggested Correction']) else row['Text']\n",
    "\n",
    "    page_mistake_counts_df = detect_missed_text_and_tables(document_path, dpi_value=400)\n",
    "    File0 = verticalOrdering(Tesseract_file)\n",
    "    File1 = removeTextonTheLeftAndright(File0, x0_Left, x0_Right)\n",
    "    File2 = merge_items_with_similar_y0(File1, tolerence)\n",
    "\n",
    "    \n",
    "    File2, header_footer_lines = remove_headers_and_footers(document_path, File0, File2, header_y0, footer_y1, auto_header_footer, h_area)\n",
    "    line_gap, y_gap_counts, total_unique_count = calculate_line_gap(File2, line_gap_margin, percentage_linegaps)\n",
    "    \n",
    "    if mergeLineclassic:\n",
    "        File3 = merge_lines(File2, line_gap)\n",
    "    else:\n",
    "        File3 = merge_linesWithPattern(File2, line_gap, patterns, bullet_patterns, table_figure_patterns)\n",
    "    \n",
    "    File4 = merge_paragraphs_with_page_break(File3)\n",
    "    File5 = add_Artificial_Headers(File4, patterns, bullet_patterns, table_figure_patterns)\n",
    "    File6 = move_headers_to_section(File5, patterns)\n",
    "    File6 = File6.reset_index(drop=True)\n",
    "    File7 = sectionType(File6, patterns)\n",
    "    File8 = sectionHierarchy(File7)\n",
    "    File9 = create_combined_section(File8)\n",
    "\n",
    "    printOnPDF(File2, File3, document_path, output_pdf_path, header_footer_lines, h_area)\n",
    "    ToExcel(File9, output_excel, \n",
    "            spell_checked_df=File9.loc[tesseract_only, :] if 'Source' in File9.columns else None, \n",
    "            page_mistake_counts_df=page_mistake_counts_df)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -- UI --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Pages for Lineboxes: 100%|██████████| 10/10 [00:00<00:00, 1111.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text with PDFMiner...\n",
      "Redacting extracted text from PDFMiner results...\n",
      "Redacted PDF saved as 'C:\\Users\\Jacqueline.chen\\Downloads\\pdfminer_redacted.pdf'.\n",
      "Extracting words with Tesseract...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Pages:   0%|          | 0/10 [00:00<?, ?page/s]C:\\Users\\Jacqueline.chen\\AppData\\Local\\Temp\\ipykernel_48504\\889963359.py:298: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  existing_words = pd.concat([existing_words, new_row], ignore_index=True)\n",
      "Processing Pages: 100%|██████████| 10/10 [01:05<00:00,  6.55s/page]\n",
      "C:\\Users\\Jacqueline.chen\\AppData\\Local\\Temp\\ipykernel_48504\\889963359.py:405: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tesseract_df['Source'] = 'Tesseract'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesseract OCR completed in 65.53 seconds\n",
      "Redacted PDF saved as 'C:\\Users\\Jacqueline.chen\\Downloads\\Pages from VN8Q_Volume_4_General_Requirements_modified redacted.pdf'.\n",
      "Intermediate files 'C:\\Users\\Jacqueline.chen\\Downloads\\pdfminer_redacted.pdf' and has been deleted.\n",
      "Final redacted PDF saved as 'C:\\Users\\Jacqueline.chen\\Downloads\\Pages from VN8Q_Volume_4_General_Requirements_modified redacted.pdf'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Spell Checking: 100%|██████████| 3/3 [00:00<00:00, 3000.22word/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: The file 'final_redacted.pdf' does not exist in C:\\Users\\Jacqueline.chen\\Downloads.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "C:\\Users\\Jacqueline.chen\\AppData\\Local\\Temp\\ipykernel_48504\\3369579384.py:5: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_sorted = df.groupby('Page', group_keys=False).apply(lambda x: x.sort_values(by='y0', ascending=False))\n",
      "C:\\Users\\Jacqueline.chen\\AppData\\Local\\Temp\\ipykernel_48504\\2787948412.py:3: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_sorted = df.groupby('Page', group_keys=False).apply(lambda x: x.sort_values(by='y0', ascending=False))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Spell check data is empty or None. No data written to 'Spell Check' sheet.\n",
      "Warning: Benchmark data is empty or None. No data written to 'Benchmark Pages with Mistakes' sheet.\n",
      "Excel file created: 'C:\\Users\\Jacqueline.chen\\Downloads\\Pages from VN8Q_Volume_4_General_Requirements_modified_Output.xlsx'.\n"
     ]
    }
   ],
   "source": [
    "## TO UPDATE FOR EACH DOCUMENT ##\n",
    "### Define Input/Output Paths ###\n",
    "FolderPath = r\"C:\\Users\\Jacqueline.chen\\Downloads\" # Path to find the PDF document to convert to RVTM\n",
    "document_name = \"Pages from VN8Q_Volume_4_General_Requirements_modified.pdf\" # Name of the PDF document to convert to RVTM\n",
    "#### Define Input PDF, Excel output and PDF with markup output path automatically based on Folder location and input PDF name\n",
    "document_path = os.path.join(FolderPath, document_name)\n",
    "output_name = os.path.splitext(document_name)[0] + \"_Output.xlsx\"\n",
    "output_pdf_name = os.path.splitext(document_name)[0] + \" image.pdf\"\n",
    "output_redacted_pdf_name = os.path.splitext(document_name)[0] + \" redacted.pdf\"\n",
    "output = os.path.join(FolderPath, output_name)\n",
    "output_pdf_path = os.path.join(FolderPath, output_pdf_name)\n",
    "\n",
    "### Trim document ###\n",
    "auto_header_footer = False #Default value is True - Program will detect header and footer auomatically. if false use header_y0 and footer_y1 will remove headers and footer\n",
    "h_area = 15 # default value is 15 - program will focus on top and botom 15% of pages to detect headers and footers (based on first page size assuming consistent page size hrough document)\n",
    "header_y0 = 720 # what is above will be removed - look at the last line of header y0 and pick a value just below it\n",
    "footer_y1 = 70 # what is below will be removed - look at the first line of footer y1 and pick a value just above it\n",
    "x0_Left = 0 # Default value is 0. What is on the left will be removed - use only when left of the pdf needs to be cropped\n",
    "x0_Right = 99999 # Default value is 99999. What is on the Right will be removed - use only when Right of the pdf needs to be cropped\n",
    "\n",
    "## TO UPDATE FOR VERY SPECIFIC DOCUMENTS - Default values should work for most documents ##\n",
    "tolerence = 1 #default value is 1 - Merge Lines with no Ycoordinate gaps within the tolerence\n",
    "line_gap_margin = 1 #Default value is 1 - Value to adjust based on PDF, it is added to line gap to include the highest values of line gap ideally if all line gaps were uniform throught the document this would be 0\n",
    "percentage_linegaps = 30 #Default Value is 30 - % estimated line gaps compared to other Y gaps in PDF (for example Paragraph gaps)\n",
    "mergeLineclassic = True #Default value is True - It means That paragraph y Gap is always bigger than line y Gaps\n",
    "is_there_a_watermark = False # True or False\n",
    "\n",
    "## TO UPDATE ONE TIME ##\n",
    "# pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\" #  #Update path with your Local Tesseract installation\n",
    "run_tesseract = True\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\Users\\Jacqueline.Chen\\AppData\\Local\\Programs\\Tesseract-OCR\\tesseract.exe\"\n",
    "\n",
    "# RUN SCRIPT #\n",
    "runScript(document_path, output, output_pdf_path, output_redacted_pdf_name, header_y0, footer_y1, auto_header_footer, h_area, x0_Left, x0_Right, tolerence, line_gap_margin, percentage_linegaps, mergeLineclassic, is_there_a_watermark, run_tesseract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runScriptPreview(\n",
    "        document_path,  # OK\n",
    "        header_y0,      # OK\n",
    "        footer_y1,      # OK\n",
    "        x0_Left,        # OK\n",
    "        x0_Right, \n",
    "        tolerence,      # OK\n",
    "        line_gap_margin, # OK\n",
    "        percentage_linegaps, # OK\n",
    "        auto_header_footer,  # OK\n",
    "        h_area):\n",
    "    # Define global variables for debugging (not generally recommended, but kept for consistency)\n",
    "    global PdfData, Tesseract_file, File0\n",
    "    global y_gap_counts\n",
    "    # Let's define a total number of steps for progress. This is arbitrary and depends on your processing:   \n",
    "    try:\n",
    "        # Start processing\n",
    "        PdfData = CreateDataFrameFromPDF(document_path)\n",
    "\n",
    "        if 'Source' not in PdfData.columns:\n",
    "            PdfData['Source'] = 'PDFMiner'\n",
    "\n",
    "        File0 = verticalOrdering(PdfData)\n",
    "        File1 = removeTextonTheLeftAndright(File0, x0_Left, x0_Right)\n",
    "        File2 = merge_items_with_similar_y0(File1, tolerence)\n",
    "        File2, header_footer_lines = remove_headers_and_footers(document_path, File0, File2, header_y0, footer_y1, auto_header_footer, h_area)\n",
    "        y_gap_counts = pd.DataFrame()\n",
    "        line_gap, y_gap_counts, total_unique_count = calculate_line_gap(File2, line_gap_margin, percentage_linegaps)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error in runScriptPreview: {e}\")\n",
    "        return False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
